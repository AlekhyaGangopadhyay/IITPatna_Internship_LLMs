{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2qWbDPWsDUM8Wjcguwiag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekhyaGangopadhyay/IITPatna_Internship_LLMs/blob/main/InternVideo2_5_4B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbyrA-NN6Xp3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  InternVideo2.5-8B  â–¸  Finance Harassment/Abuse Dataset\n",
        "#  Full Pipeline : Train (LoRA) â†’ Infer â†’ Accuracy Score\n",
        "#  Dataset       : iamalekhya/Finance_set  (318 videos, 9 cols)\n",
        "#\n",
        "#  InternVideo2.5 is built ON TOP of InternVL2.5.\n",
        "#  It adds HiCo token compression + TPO for long video understanding.\n",
        "#  Uses the same model.chat() API as InternVL2.5 but adds\n",
        "#  num_patches_list to handle dynamic tiling per frame.\n",
        "#\n",
        "#  GPU:  ~16 GB in bfloat16  â†’  T4 free Colab âœ…  (with 4-bit)\n",
        "#        ~14 GB in 4-bit     â†’  T4 free Colab âœ…  â† recommended\n",
        "# ============================================================\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â”€â”€ Install  (run once â†’ Runtime â–¸ Restart session)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# IMPORTANT: transformers must be pinned to 4.40.1 for InternVideo2.5\n",
        "!pip install \"transformers==4.40.1\" accelerate sentencepiece peft bitsandbytes\n",
        "!pip install decord av imageio opencv-python\n",
        "!pip install rouge-score openpyxl scikit-learn\n",
        "!pip install flash-attn --no-build-isolation   # optional but recommended\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 2 â”€â”€ Mount Google Drive\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 3 â”€â”€ âš™ï¸  CONFIG  â† only edit this section\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "MODEL_PATH = \"OpenGVLab/InternVideo2_5_Chat_4B\"\n",
        "\n",
        "# Dataset\n",
        "HF_DATASET     = \"iamalekhya/Finance_set\"\n",
        "EXCEL_FALLBACK = \"/content/drive/MyDrive/IITP_Internship/Finance_set.xlsx\"\n",
        "\n",
        "# Videos in Google Drive\n",
        "VIDEO_DIRS = [\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/AG\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/ag\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Alekhya\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Pritam\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Videos\",\n",
        "]\n",
        "VIDEO_EXT = \".mp4\"\n",
        "\n",
        "# Training\n",
        "# InternVideo2.5 uses dynamic tiling: each frame â†’ multiple tiles (num_patches_list)\n",
        "# max_num=1 means 1 tile + 1 thumbnail = 2 patches per frame â†’ safer for T4\n",
        "NUM_SEGMENTS = 16    # number of frames to sample from each video\n",
        "MAX_NUM      = 1     # max dynamic tiles per frame  (1 = safe for T4, up to 6 for A100)\n",
        "IMAGE_SIZE   = 448   # InternVideo2.5 native resolution\n",
        "MAX_SEQ_LEN  = 2048\n",
        "EPOCHS       = 3\n",
        "BATCH_SIZE   = 1\n",
        "GRAD_ACCUM   = 8\n",
        "LORA_RANK    = 16\n",
        "EVAL_SPLIT   = 0.10\n",
        "RANDOM_SEED  = 42\n",
        "\n",
        "# Paths\n",
        "OUTPUT_DIR          = \"./internvideo25_finance\"\n",
        "ADAPTER_DRIVE       = \"/content/drive/MyDrive/IIT_Internship/internvideo25_8b_adapter\"\n",
        "RESULTS_EXCEL_DRIVE = \"/content/drive/MyDrive/IIT_Internship/internvideo25_8b_results.xlsx\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 4 â”€â”€ Imports\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "import os, re, json, shutil, torch, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rouge_score import rouge_scorer as rs\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "import decord\n",
        "decord.bridge.set_bridge(\"torch\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"âœ… Imports OK\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 5 â”€â”€ Official InternVideo2.5 Image Preprocessing\n",
        "#            (dynamic tiling â€” official code from HuggingFace)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "def build_transform(input_size=IMAGE_SIZE):\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "TRANSFORM = build_transform(IMAGE_SIZE)\n",
        "\n",
        "\n",
        "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
        "    best_ratio_diff = float(\"inf\")\n",
        "    best_ratio = (1, 1)\n",
        "    area = width * height\n",
        "    for ratio in target_ratios:\n",
        "        target_aspect_ratio = ratio[0] / ratio[1]\n",
        "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
        "        if ratio_diff < best_ratio_diff:\n",
        "            best_ratio_diff = ratio_diff\n",
        "            best_ratio = ratio\n",
        "        elif ratio_diff == best_ratio_diff:\n",
        "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
        "                best_ratio = ratio\n",
        "    return best_ratio\n",
        "\n",
        "\n",
        "def dynamic_preprocess(image, min_num=1, max_num=MAX_NUM,\n",
        "                        image_size=IMAGE_SIZE, use_thumbnail=True):\n",
        "    \"\"\"\n",
        "    Split a PIL image into dynamic tiles based on aspect ratio.\n",
        "    Always adds a thumbnail tile when use_thumbnail=True.\n",
        "    Returns list of PIL tile images.\n",
        "    \"\"\"\n",
        "    orig_width, orig_height = image.size\n",
        "    aspect_ratio = orig_width / orig_height\n",
        "\n",
        "    target_ratios = set(\n",
        "        (i, j)\n",
        "        for n in range(min_num, max_num + 1)\n",
        "        for i in range(1, n + 1)\n",
        "        for j in range(1, n + 1)\n",
        "        if min_num <= i * j <= max_num\n",
        "    )\n",
        "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
        "    target_aspect_ratio = find_closest_aspect_ratio(\n",
        "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
        "    )\n",
        "\n",
        "    target_width  = image_size * target_aspect_ratio[0]\n",
        "    target_height = image_size * target_aspect_ratio[1]\n",
        "    blocks        = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
        "\n",
        "    resized_img       = image.resize((target_width, target_height))\n",
        "    processed_images  = []\n",
        "\n",
        "    for i in range(blocks):\n",
        "        cols = target_width // image_size\n",
        "        box  = (\n",
        "            (i % cols) * image_size,\n",
        "            (i // cols) * image_size,\n",
        "            ((i % cols) + 1) * image_size,\n",
        "            ((i // cols) + 1) * image_size,\n",
        "        )\n",
        "        processed_images.append(resized_img.crop(box))\n",
        "\n",
        "    if use_thumbnail and len(processed_images) != 1:\n",
        "        processed_images.append(image.resize((image_size, image_size)))\n",
        "\n",
        "    return processed_images\n",
        "\n",
        "\n",
        "def process_pil_frame(pil_img, max_num=MAX_NUM) -> tuple:\n",
        "    \"\"\"\n",
        "    Convert a single PIL frame â†’ (pixel_values_tensor, num_patches).\n",
        "    num_patches = number of tiles generated for this frame.\n",
        "    \"\"\"\n",
        "    tiles        = dynamic_preprocess(pil_img, max_num=max_num, use_thumbnail=True)\n",
        "    pixel_values = torch.stack([TRANSFORM(tile) for tile in tiles])\n",
        "    return pixel_values, pixel_values.shape[0]   # (n_tiles, 3, H, W), n_tiles\n",
        "\n",
        "\n",
        "def load_video_internvideo(video_path: str,\n",
        "                            num_segments: int = NUM_SEGMENTS,\n",
        "                            max_num: int = MAX_NUM) -> tuple:\n",
        "    \"\"\"\n",
        "    Official InternVideo2.5 video loading using decord.\n",
        "    Returns (pixel_values, num_patches_list):\n",
        "      pixel_values     : (total_tiles, 3, H, W)\n",
        "      num_patches_list : list of tile counts per frame â€” required by model.chat()\n",
        "    \"\"\"\n",
        "    vr        = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
        "    max_frame = len(vr) - 1\n",
        "    fps       = float(vr.get_avg_fps())\n",
        "\n",
        "    seg_size     = float(max_frame) / num_segments\n",
        "    frame_indices = np.array([\n",
        "        int(seg_size / 2 + np.round(seg_size * i))\n",
        "        for i in range(num_segments)\n",
        "    ])\n",
        "    frame_indices = np.clip(frame_indices, 0, max_frame)\n",
        "\n",
        "    pixel_values_list = []\n",
        "    num_patches_list  = []\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        pil_img     = Image.fromarray(vr[idx].asnumpy()).convert(\"RGB\")\n",
        "        pv, n_tiles = process_pil_frame(pil_img, max_num=max_num)\n",
        "        pixel_values_list.append(pv)\n",
        "        num_patches_list.append(n_tiles)\n",
        "\n",
        "    pixel_values = torch.cat(pixel_values_list, dim=0)   # (total_tiles, 3, H, W)\n",
        "    return pixel_values, num_patches_list\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 6 â”€â”€ Load Dataset\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    ds = load_dataset(HF_DATASET, split=\"train\")\n",
        "    df = ds.to_pandas()\n",
        "    print(f\"âœ… Loaded from HuggingFace: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  HF failed ({e}), loading from Excel...\")\n",
        "    df = pd.read_excel(EXCEL_FALLBACK)\n",
        "    print(f\"âœ… Loaded from Excel: {len(df)} rows\")\n",
        "\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "REQUIRED = [\"uni_id\",\"Category\",\"Aspect\",\"Intent\",\"Explanation\",\"Country\",\"Source\",\"Sector\"]\n",
        "missing  = [c for c in REQUIRED if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "df = df.dropna(subset=REQUIRED).reset_index(drop=True)\n",
        "print(f\"   Rows: {len(df)} | Categories: {df['Category'].unique().tolist()}\")\n",
        "\n",
        "ALL_ASPECTS = sorted(df[\"Aspect\"].unique().tolist())\n",
        "print(f\"   Aspects: {len(ALL_ASPECTS)} unique values\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 â”€â”€ Prompt\n",
        "#\n",
        "#  InternVideo2.5 video format (official):\n",
        "#    video_prefix = \"Frame1: <image>\\nFrame2: <image>\\n...\"\n",
        "#    question     = video_prefix + instruction_text\n",
        "#    num_patches_list is passed to model.chat() separately\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "def build_question(num_frames: int) -> str:\n",
        "    \"\"\"\n",
        "    Build question string with Frame{i}: <image> prefix for each frame.\n",
        "    This is the EXACT format required by InternVideo2.5.\n",
        "    \"\"\"\n",
        "    video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(num_frames)])\n",
        "\n",
        "    instruction = f\"\"\"You are an expert video analyst specialising in financial harassment, fraud, abuse, and workplace misconduct.\n",
        "\n",
        "The frames above are sampled evenly from a video. Analyse them carefully and return a single valid JSON object with EXACTLY these fields:\n",
        "{{\n",
        "  \"category\"   : \"Harassment\" | \"Bullying\",\n",
        "  \"aspect\"     : one of the known aspect labels listed below,\n",
        "  \"intent\"     : 1 (intentional) | 0 (unintentional),\n",
        "  \"explanation\": \"1-2 sentence description of what is happening and why it is problematic\",\n",
        "  \"country\"    : \"country name or Unknown\",\n",
        "  \"source\"     : \"YouTube | News | Training | Documentary | Other\",\n",
        "  \"sector\"     : \"Finance | Healthcare | Education | Retail | Tech | Government | General\"\n",
        "}}\n",
        "\n",
        "Known aspect labels (pick the closest match):\n",
        "{json.dumps(ALL_ASPECTS, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the JSON. No preamble, no markdown fences, no extra text.\n",
        "- Use double quotes for all strings.\n",
        "- intent must be an integer (1 or 0), not a string.\n",
        "\n",
        "Observe carefully:\n",
        "â€¢ Who is involved and what is happening\n",
        "â€¢ The type and form of misconduct\n",
        "â€¢ Whether the act appears deliberate\n",
        "â€¢ Country and industry context clues\"\"\"\n",
        "\n",
        "    return video_prefix + instruction\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 8 â”€â”€ Helpers\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "def find_video(uni_id: str) -> str:\n",
        "    filename = f\"{uni_id}{VIDEO_EXT}\"\n",
        "    for folder in VIDEO_DIRS:\n",
        "        path = os.path.join(folder, filename)\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "        if os.path.isdir(folder):\n",
        "            for f in os.listdir(folder):\n",
        "                if f.lower() == filename.lower():\n",
        "                    return os.path.join(folder, f)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def row_to_json(row) -> str:\n",
        "    return json.dumps({\n",
        "        \"category\"   : str(row[\"Category\"]).strip(),\n",
        "        \"aspect\"     : str(row[\"Aspect\"]).strip(),\n",
        "        \"intent\"     : int(row[\"Intent\"]),\n",
        "        \"explanation\": str(row[\"Explanation\"]).strip(),\n",
        "        \"country\"    : str(row[\"Country\"]).strip(),\n",
        "        \"source\"     : str(row[\"Source\"]).strip(),\n",
        "        \"sector\"     : str(row[\"Sector\"]).strip(),\n",
        "    }, indent=2)\n",
        "\n",
        "\n",
        "def parse_json_response(text: str) -> dict:\n",
        "    try:\n",
        "        m = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if m:\n",
        "            return json.loads(m.group())\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 9 â”€â”€ Load Model + Tokenizer + LoRA\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(f\"\\nLoading {MODEL_PATH}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    trust_remote_code = True,\n",
        "    use_fast          = False,\n",
        ")\n",
        "\n",
        "# 4-bit quantisation â€” fits on free T4 (16 GB)\n",
        "# For A100: remove quant_config and use torch_dtype=torch.bfloat16\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_compute_dtype    = torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    quantization_config = quant_config,\n",
        "    trust_remote_code   = True,\n",
        "    low_cpu_mem_usage   = True,\n",
        ").eval()\n",
        "\n",
        "print(\"âœ… Model loaded\")\n",
        "\n",
        "# LoRA on the InternLM2 language backbone\n",
        "# InternVideo2.5 language model is an InternLM2-7B accessed via .language_model\n",
        "lora_config = LoraConfig(\n",
        "    task_type      = TaskType.CAUSAL_LM,\n",
        "    r              = LORA_RANK,\n",
        "    lora_alpha     = LORA_RANK * 2,\n",
        "    lora_dropout   = 0.05,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias           = \"none\",\n",
        ")\n",
        "model.language_model = get_peft_model(model.language_model, lora_config)\n",
        "model.language_model.print_trainable_parameters()\n",
        "print(\"âœ… LoRA applied\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 10 â”€â”€ Build Samples\n",
        "#             Load video via decord â†’ get pixel_values + num_patches_list\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "train_df, eval_df = train_test_split(df, test_size=EVAL_SPLIT, random_state=RANDOM_SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "eval_df  = eval_df.reset_index(drop=True)\n",
        "print(f\"\\nSplit â†’ Train: {len(train_df)}  |  Eval: {len(eval_df)}\")\n",
        "\n",
        "\n",
        "def build_samples(dataframe, split_name=\"train\"):\n",
        "    samples, skipped = [], 0\n",
        "    for i, row in dataframe.iterrows():\n",
        "        uni_id     = str(row[\"uni_id\"]).strip()\n",
        "        video_path = find_video(uni_id)\n",
        "\n",
        "        if not video_path:\n",
        "            print(f\"  âš ï¸  [{split_name}] Skipping {uni_id} â€” video not found\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pixel_values, num_patches_list = load_video_internvideo(\n",
        "                video_path, num_segments=NUM_SEGMENTS, max_num=MAX_NUM\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸  [{split_name}] Skipping {uni_id} â€” decord error: {e}\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        target = row_to_json(row)\n",
        "\n",
        "        samples.append({\n",
        "            \"pixel_values\"    : pixel_values,       # (total_tiles, 3, H, W) float32 tensor\n",
        "            \"num_patches_list\": num_patches_list,    # [n_tiles_per_frame, ...]\n",
        "            \"question\"        : build_question(len(num_patches_list)),\n",
        "            \"target_json\"     : target,\n",
        "            \"uni_id\"          : uni_id,\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 30 == 0:\n",
        "            print(f\"  [{split_name}] {i+1}/{len(dataframe)}  skipped={skipped}\")\n",
        "\n",
        "    print(f\"  âœ… [{split_name}] ready={len(samples)}  skipped={skipped}\")\n",
        "    return samples\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“¦ Building training samples (loading videos with decord)...\")\n",
        "train_samples = build_samples(train_df, \"train\")\n",
        "\n",
        "print(\"\\nğŸ“¦ Building eval samples...\")\n",
        "eval_samples  = build_samples(eval_df,  \"eval\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 11 â”€â”€ PyTorch Dataset + Collator\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "IMG_CONTEXT_TOKEN = '<IMG_CONTEXT>'\n",
        "IMG_TOKEN         = '<image>'\n",
        "\n",
        "class InternVideo25Dataset(TorchDataset):\n",
        "    \"\"\"\n",
        "    Tokenises using InternVL/InternVideo2.5 chat template.\n",
        "\n",
        "    Each <image> in the question must be replaced by\n",
        "    num_image_token copies of <IMG_CONTEXT> for that specific frame.\n",
        "    Because InternVideo2.5 uses dynamic tiling, different frames may\n",
        "    have different num_patches, so we expand frame-by-frame.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, tokenizer, model, max_length=2048):\n",
        "        self.samples         = samples\n",
        "        self.tokenizer       = tokenizer\n",
        "        self.max_length      = max_length\n",
        "        self.num_image_token = model.num_image_token  # 256 for ViT-448\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s                = self.samples[idx]\n",
        "        pixel_values     = s[\"pixel_values\"]       # (total_tiles, 3, H, W)\n",
        "        num_patches_list = s[\"num_patches_list\"]   # [tiles_per_frame, ...]\n",
        "        question         = s[\"question\"]\n",
        "        answer           = s[\"target_json\"]\n",
        "\n",
        "        # Expand each <image> tag with the correct number of IMG_CONTEXT tokens\n",
        "        # for that specific frame (dynamic tiling means each frame may differ)\n",
        "        expanded_q = question\n",
        "        for n_patches in num_patches_list:\n",
        "            expanded_q = expanded_q.replace(\n",
        "                IMG_TOKEN,\n",
        "                IMG_CONTEXT_TOKEN * self.num_image_token * n_patches,\n",
        "                1   # replace only the first occurrence each iteration\n",
        "            )\n",
        "\n",
        "        SYSTEM = (\"You are an expert video analyst specialising in financial \"\n",
        "                  \"harassment, fraud, abuse, and workplace misconduct. \"\n",
        "                  \"Always respond with valid JSON only.\")\n",
        "\n",
        "        full_text = (\n",
        "            f\"<|im_start|>system\\n{SYSTEM}<|im_end|>\"\n",
        "            f\"<|im_start|>user\\n{expanded_q}<|im_end|>\"\n",
        "            f\"<|im_start|>assistant\\n{answer}<|im_end|>\"\n",
        "        )\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            return_tensors = \"pt\",\n",
        "            max_length     = self.max_length,\n",
        "            truncation     = True,\n",
        "            padding        = False,\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        # Labels: mask everything up to (and including) assistant header\n",
        "        labels = input_ids.clone()\n",
        "        assistant_header = self.tokenizer.encode(\n",
        "            \"<|im_start|>assistant\\n\", add_special_tokens=False\n",
        "        )\n",
        "        header_len = len(assistant_header)\n",
        "        mask_until = 0\n",
        "        for pos in range(len(input_ids) - header_len):\n",
        "            if input_ids[pos : pos + header_len].tolist() == assistant_header:\n",
        "                mask_until = pos + header_len\n",
        "\n",
        "        labels[:mask_until] = -100\n",
        "        pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\"     : input_ids,\n",
        "            \"attention_mask\": torch.ones_like(input_ids),\n",
        "            \"labels\"        : labels,\n",
        "            \"pixel_values\"  : pixel_values,    # kept as-is for collator\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "\n",
        "    input_ids      = pad_sequence([b[\"input_ids\"]      for b in batch], batch_first=True, padding_value=pad_id)\n",
        "    attention_mask = pad_sequence([b[\"attention_mask\"] for b in batch], batch_first=True, padding_value=0)\n",
        "    labels         = pad_sequence([b[\"labels\"]         for b in batch], batch_first=True, padding_value=-100)\n",
        "\n",
        "    # pixel_values: each sample may have different total_tiles\n",
        "    # Concatenate along dim=0 (tile dim)\n",
        "    pixel_values = torch.cat([b[\"pixel_values\"] for b in batch], dim=0)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\"     : input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\"        : labels,\n",
        "        \"pixel_values\"  : pixel_values,\n",
        "    }\n",
        "\n",
        "\n",
        "train_torch_ds = InternVideo25Dataset(train_samples, tokenizer, model, max_length=MAX_SEQ_LEN)\n",
        "print(f\"âœ… PyTorch dataset ready: {len(train_torch_ds)} training samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 12 â”€â”€ Train\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "model.train()\n",
        "model.language_model.enable_input_require_grads()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                    = OUTPUT_DIR,\n",
        "    per_device_train_batch_size   = BATCH_SIZE,\n",
        "    gradient_accumulation_steps   = GRAD_ACCUM,\n",
        "    num_train_epochs              = EPOCHS,\n",
        "    learning_rate                 = 2e-4,\n",
        "    bf16                          = True,\n",
        "    logging_steps                 = 5,\n",
        "    save_steps                    = 100,\n",
        "    save_total_limit              = 2,\n",
        "    warmup_ratio                  = 0.03,\n",
        "    lr_scheduler_type             = \"cosine\",\n",
        "    optim                         = \"paged_adamw_8bit\",\n",
        "    seed                          = RANDOM_SEED,\n",
        "    remove_unused_columns         = False,\n",
        "    dataloader_pin_memory         = False,\n",
        "    gradient_checkpointing        = True,\n",
        "    gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
        "    report_to                     = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_torch_ds,\n",
        "    data_collator = collate_fn,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ Training started...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.language_model.save_pretrained(\"internvideo25_lora_adapter\")\n",
        "tokenizer.save_pretrained(\"internvideo25_lora_adapter\")\n",
        "os.makedirs(ADAPTER_DRIVE, exist_ok=True)\n",
        "shutil.copytree(\"internvideo25_lora_adapter\", ADAPTER_DRIVE, dirs_exist_ok=True)\n",
        "print(f\"âœ… LoRA adapter saved â†’ {ADAPTER_DRIVE}\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 13 â”€â”€ Inference\n",
        "#\n",
        "#  Official InternVideo2.5 inference:\n",
        "#    model.chat(tokenizer, pixel_values, question,\n",
        "#               generation_config, num_patches_list=..., history=None)\n",
        "#  num_patches_list is MANDATORY â€” it tells the model how many\n",
        "#  tiles belong to each frame for correct attention masking.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "model.eval()\n",
        "\n",
        "GENERATION_CONFIG = dict(\n",
        "    do_sample      = False,\n",
        "    temperature    = 0.0,\n",
        "    max_new_tokens = 512,\n",
        "    top_p          = 0.1,\n",
        "    num_beams      = 1,\n",
        ")\n",
        "\n",
        "def predict(video_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Run InternVideo2.5 inference on a video file.\n",
        "    Uses decord for frame extraction + dynamic tiling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pixel_values, num_patches_list = load_video_internvideo(\n",
        "            video_path, num_segments=NUM_SEGMENTS, max_num=MAX_NUM\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸  Decord error: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # Cast to bfloat16 and move to model device\n",
        "    pixel_values = pixel_values.to(dtype=torch.bfloat16, device=model.device)\n",
        "\n",
        "    question = build_question(len(num_patches_list))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        response, _ = model.chat(\n",
        "            tokenizer         = tokenizer,\n",
        "            pixel_values      = pixel_values,\n",
        "            question          = question,\n",
        "            generation_config = GENERATION_CONFIG,\n",
        "            num_patches_list  = num_patches_list,  # REQUIRED for InternVideo2.5\n",
        "            history           = None,\n",
        "            return_history    = True,\n",
        "        )\n",
        "\n",
        "    return parse_json_response(response)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 14 â”€â”€ Run Inference on ALL samples (train + eval)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"  ğŸ” Running inference on ALL samples (train + eval)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "all_samples = train_samples + eval_samples\n",
        "all_results = []\n",
        "\n",
        "for i, s in enumerate(all_samples):\n",
        "    video_path = find_video(s[\"uni_id\"])\n",
        "    pred = predict(video_path)\n",
        "    tgt  = json.loads(s[\"target_json\"])\n",
        "\n",
        "    all_results.append({\n",
        "        \"uni_id\"          : s[\"uni_id\"],\n",
        "        \"split\"           : \"train\" if i < len(train_samples) else \"eval\",\n",
        "        \"true_category\"   : tgt.get(\"category\",    \"\"),\n",
        "        \"true_aspect\"     : tgt.get(\"aspect\",       \"\"),\n",
        "        \"true_intent\"     : str(tgt.get(\"intent\",   \"\")),\n",
        "        \"true_explanation\": tgt.get(\"explanation\",  \"\"),\n",
        "        \"true_country\"    : tgt.get(\"country\",      \"\"),\n",
        "        \"true_source\"     : tgt.get(\"source\",       \"\"),\n",
        "        \"true_sector\"     : tgt.get(\"sector\",       \"\"),\n",
        "        \"pred_category\"   : str(pred.get(\"category\",    \"\")).strip(),\n",
        "        \"pred_aspect\"     : str(pred.get(\"aspect\",       \"\")).strip(),\n",
        "        \"pred_intent\"     : str(pred.get(\"intent\",       \"\")).strip(),\n",
        "        \"pred_explanation\": str(pred.get(\"explanation\",  \"\")).strip(),\n",
        "        \"pred_country\"    : str(pred.get(\"country\",      \"\")).strip(),\n",
        "        \"pred_source\"     : str(pred.get(\"source\",       \"\")).strip(),\n",
        "        \"pred_sector\"     : str(pred.get(\"sector\",       \"\")).strip(),\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 20 == 0:\n",
        "        print(f\"  Inferred {i+1}/{len(all_samples)}\")\n",
        "\n",
        "print(f\"âœ… Inference done for {len(all_results)} samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 15 â”€â”€ Accuracy Scoring\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "scorer = rs.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def score_split(rows, label=\"ALL\"):\n",
        "    if not rows:\n",
        "        return {}\n",
        "\n",
        "    def acc(t, p):\n",
        "        return accuracy_score(\n",
        "            [x.lower().strip() for x in t],\n",
        "            [x.lower().strip() for x in p]\n",
        "        )\n",
        "\n",
        "    rouge_scores = [\n",
        "        scorer.score(r[\"true_explanation\"], r[\"pred_explanation\"])[\"rougeL\"].fmeasure\n",
        "        for r in rows\n",
        "    ]\n",
        "    avg_rouge = float(np.mean(rouge_scores))\n",
        "\n",
        "    cat_acc = acc([r[\"true_category\"] for r in rows], [r[\"pred_category\"] for r in rows])\n",
        "    asp_acc = acc([r[\"true_aspect\"]   for r in rows], [r[\"pred_aspect\"]   for r in rows])\n",
        "    int_acc = acc([r[\"true_intent\"]   for r in rows], [r[\"pred_intent\"]   for r in rows])\n",
        "    cty_acc = acc([r[\"true_country\"]  for r in rows], [r[\"pred_country\"]  for r in rows])\n",
        "    sec_acc = acc([r[\"true_sector\"]   for r in rows], [r[\"pred_sector\"]   for r in rows])\n",
        "    src_acc = acc([r[\"true_source\"]   for r in rows], [r[\"pred_source\"]   for r in rows])\n",
        "\n",
        "    overall = (cat_acc*0.25 + asp_acc*0.25 + int_acc*0.15 +\n",
        "               avg_rouge*0.20 + sec_acc*0.10 + cty_acc*0.03 + src_acc*0.02)\n",
        "\n",
        "    return {\n",
        "        \"split\"             : label,\n",
        "        \"n_samples\"         : len(rows),\n",
        "        \"category_acc\"      : round(cat_acc   * 100, 2),\n",
        "        \"aspect_acc\"        : round(asp_acc   * 100, 2),\n",
        "        \"intent_acc\"        : round(int_acc   * 100, 2),\n",
        "        \"country_acc\"       : round(cty_acc   * 100, 2),\n",
        "        \"source_acc\"        : round(src_acc   * 100, 2),\n",
        "        \"sector_acc\"        : round(sec_acc   * 100, 2),\n",
        "        \"explanation_rougeL\": round(avg_rouge * 100, 2),\n",
        "        \"OVERALL_SCORE_%\"   : round(overall   * 100, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "train_rows   = [r for r in all_results if r[\"split\"] == \"train\"]\n",
        "eval_rows    = [r for r in all_results if r[\"split\"] == \"eval\"]\n",
        "train_scores = score_split(train_rows,  \"TRAIN\")\n",
        "eval_scores  = score_split(eval_rows,   \"EVAL\")\n",
        "all_scores   = score_split(all_results, \"ALL\")\n",
        "\n",
        "def print_scores(s: dict):\n",
        "    print(f\"\\n  â”€â”€ {s['split']} SET  ({s['n_samples']} samples) â”€â”€\")\n",
        "    print(f\"   Category    Accuracy  : {s['category_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Aspect      Accuracy  : {s['aspect_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Intent      Accuracy  : {s['intent_acc']:>7.2f} %   (weight 15%)\")\n",
        "    print(f\"   Explanation ROUGE-L   : {s['explanation_rougeL']:>7.2f} %   (weight 20%)\")\n",
        "    print(f\"   Sector      Accuracy  : {s['sector_acc']:>7.2f} %   (weight 10%)\")\n",
        "    print(f\"   Country     Accuracy  : {s['country_acc']:>7.2f} %   (weight  3%)\")\n",
        "    print(f\"   Source      Accuracy  : {s['source_acc']:>7.2f} %   (weight  2%)\")\n",
        "    print(f\"   {'â”€'*42}\")\n",
        "    print(f\"   â­ OVERALL SCORE      : {s['OVERALL_SCORE_%']:>7.2f} %\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"              ğŸ“Š ACCURACY REPORT\")\n",
        "print(\"=\"*65)\n",
        "print_scores(train_scores)\n",
        "print_scores(eval_scores)\n",
        "print_scores(all_scores)\n",
        "print(\"=\"*65)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 16 â”€â”€ Save Results to Excel (7 sheets)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "results_df = pd.DataFrame(all_results)\n",
        "summary_df = pd.DataFrame([train_scores, eval_scores, all_scores])\n",
        "\n",
        "os.makedirs(os.path.dirname(RESULTS_EXCEL_DRIVE), exist_ok=True)\n",
        "\n",
        "with pd.ExcelWriter(RESULTS_EXCEL_DRIVE, engine=\"openpyxl\") as writer:\n",
        "    summary_df.to_excel(writer, sheet_name=\"Accuracy Summary\", index=False)\n",
        "    results_df.to_excel(writer, sheet_name=\"Per Sample\",       index=False)\n",
        "\n",
        "    for field in [\"category\", \"aspect\", \"intent\", \"sector\", \"country\", \"source\"]:\n",
        "        breakdown = results_df[[\n",
        "            \"uni_id\", \"split\",\n",
        "            f\"true_{field}\", f\"pred_{field}\"\n",
        "        ]].copy()\n",
        "        breakdown[\"correct\"] = (\n",
        "            breakdown[f\"true_{field}\"].str.lower().str.strip() ==\n",
        "            breakdown[f\"pred_{field}\"].str.lower().str.strip()\n",
        "        )\n",
        "        breakdown.to_excel(writer, sheet_name=f\"{field.capitalize()} Detail\", index=False)\n",
        "\n",
        "print(f\"\\nâœ… Results saved â†’ {RESULTS_EXCEL_DRIVE}\")\n",
        "print(\"\\nğŸ‰ ALL DONE!\")"
      ]
    }
  ]
}