{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuoliT9gRnL9NtdISwRtvb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekhyaGangopadhyay/IITPatna_Internship_LLMs/blob/main/InternVL2_5_8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGkGnvcF3GsV"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  InternVL2.5-8B  â–¸  Finance Harassment/Abuse Video Dataset\n",
        "#  Full Pipeline : Train (LoRA) â†’ Infer â†’ Accuracy Score\n",
        "#  Dataset       : iamalekhya/Finance_set  (318 videos, 9 cols)\n",
        "#  Run on        : Google Colab FREE T4 (16 GB VRAM) âœ…\n",
        "# ============================================================\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â”€â”€ Install  (run once â†’ Runtime â–¸ Restart session)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "!pip install transformers>=4.37.2 accelerate bitsandbytes peft\n",
        "!pip install rouge-score openpyxl scikit-learn opencv-python timm\n",
        "!pip install flash-attn --no-build-isolation   # optional, speeds up attention on A10/A100\n",
        "# After install â†’ Runtime â–¸ Restart session â–¸ then run Cell 2 onwards\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 2 â”€â”€ Mount Google Drive\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 3 â”€â”€ âš™ï¸  CONFIG  â† only edit this section\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "MODEL_PATH     = \"OpenGVLab/InternVL2_5-8B\"    # fits on free T4 in 4-bit\n",
        "\n",
        "# Dataset\n",
        "HF_DATASET     = \"iamalekhya/Finance_set\"\n",
        "EXCEL_FALLBACK = \"/content/drive/MyDrive/IITP_Internship/Finance_set.xlsx\"\n",
        "\n",
        "# Videos in Google Drive (all 5 folders searched automatically)\n",
        "VIDEO_DIRS = [\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/AG\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/ag\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Alekhya\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Pritam\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Videos\",\n",
        "]\n",
        "VIDEO_EXT = \".mp4\"\n",
        "\n",
        "# Training\n",
        "NUM_FRAMES  = 8       # frames per video  (reduce to 4 if OOM on T4)\n",
        "IMAGE_SIZE  = 448     # InternVL2.5 native resolution\n",
        "MAX_SEQ_LEN = 2048\n",
        "EPOCHS      = 3\n",
        "BATCH_SIZE  = 1\n",
        "GRAD_ACCUM  = 8\n",
        "LORA_RANK   = 16\n",
        "EVAL_SPLIT  = 0.10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Paths\n",
        "OUTPUT_DIR          = \"./internvl25_8b_finance\"\n",
        "ADAPTER_DRIVE       = \"/content/drive/MyDrive/IIT_Internship/internvl25_8b_adapter\"\n",
        "RESULTS_EXCEL_DRIVE = \"/content/drive/MyDrive/IIT_Internship/internvl25_8b_results.xlsx\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 4 â”€â”€ Imports\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "import os, re, json, cv2, shutil, torch, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rouge_score import rouge_scorer as rs\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"âœ… Imports OK\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 5 â”€â”€ Image Transform  (official InternVL2.5 pipeline)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "def build_transform(size=IMAGE_SIZE):\n",
        "    return T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((size, size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "IMG_TRANSFORM = build_transform(IMAGE_SIZE)\n",
        "\n",
        "def pil_to_pixel_values(pil_images: list) -> torch.Tensor:\n",
        "    \"\"\"Stack PIL frames â†’ (N, 3, H, W) float tensor.\"\"\"\n",
        "    return torch.stack([IMG_TRANSFORM(img) for img in pil_images])\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 6 â”€â”€ Load Dataset\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    ds = load_dataset(HF_DATASET, split=\"train\")\n",
        "    df = ds.to_pandas()\n",
        "    print(f\"âœ… Loaded from HuggingFace: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  HF failed ({e}), loading from Excel...\")\n",
        "    df = pd.read_excel(EXCEL_FALLBACK)\n",
        "    print(f\"âœ… Loaded from Excel: {len(df)} rows\")\n",
        "\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "REQUIRED = [\"uni_id\",\"Category\",\"Aspect\",\"Intent\",\"Explanation\",\"Country\",\"Source\",\"Sector\"]\n",
        "missing  = [c for c in REQUIRED if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "df = df.dropna(subset=REQUIRED).reset_index(drop=True)\n",
        "print(f\"   Rows: {len(df)} | Categories: {df['Category'].unique().tolist()}\")\n",
        "\n",
        "ALL_ASPECTS = sorted(df[\"Aspect\"].unique().tolist())\n",
        "print(f\"   Aspects: {len(ALL_ASPECTS)} unique values\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 â”€â”€ Prompt\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an expert video analyst specialising in financial harassment, \"\n",
        "    \"fraud, abuse, and workplace misconduct. Always respond with valid JSON only.\"\n",
        ")\n",
        "\n",
        "def build_question(n_frames: int) -> str:\n",
        "    \"\"\"\n",
        "    InternVL2.5 multi-image format:\n",
        "    Each frame needs exactly one <image> tag in the question string.\n",
        "    \"\"\"\n",
        "    frame_tags = \"\\n\".join([f\"Frame {i+1}: <image>\" for i in range(n_frames)])\n",
        "    return f\"\"\"{frame_tags}\n",
        "\n",
        "Analyse the video frames above carefully and return a single valid JSON object with EXACTLY these fields:\n",
        "{{\n",
        "  \"category\"   : \"Harassment\" | \"Bullying\",\n",
        "  \"aspect\"     : one of the known aspect labels listed below,\n",
        "  \"intent\"     : 1 (intentional) | 0 (unintentional),\n",
        "  \"explanation\": \"1-2 sentence description of what is happening and why it is problematic\",\n",
        "  \"country\"    : \"country name or Unknown\",\n",
        "  \"source\"     : \"YouTube | News | Training | Documentary | Other\",\n",
        "  \"sector\"     : \"Finance | Healthcare | Education | Retail | Tech | Government | General\"\n",
        "}}\n",
        "\n",
        "Known aspect labels (pick the closest match):\n",
        "{json.dumps(ALL_ASPECTS, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the JSON. No preamble, no markdown fences, no extra text.\n",
        "- Use double quotes for all strings.\n",
        "- intent must be an integer (1 or 0), not a string.\n",
        "\n",
        "Observe carefully:\n",
        "â€¢ Who is involved and what is happening\n",
        "â€¢ The type and form of misconduct\n",
        "â€¢ Whether the act appears deliberate\n",
        "â€¢ Country and industry context clues\"\"\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 8 â”€â”€ Helpers\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "def find_video(uni_id: str) -> str:\n",
        "    filename = f\"{uni_id}{VIDEO_EXT}\"\n",
        "    for folder in VIDEO_DIRS:\n",
        "        path = os.path.join(folder, filename)\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "        if os.path.isdir(folder):\n",
        "            for f in os.listdir(folder):\n",
        "                if f.lower() == filename.lower():\n",
        "                    return os.path.join(folder, f)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_frames(video_path: str, n: int = NUM_FRAMES) -> list:\n",
        "    if not video_path or not os.path.exists(video_path):\n",
        "        return []\n",
        "    cap   = cv2.VideoCapture(video_path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    frames = []\n",
        "    for idx in np.linspace(0, total - 1, n, dtype=int):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, frame = cap.read()\n",
        "        if ok:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def row_to_json(row) -> str:\n",
        "    return json.dumps({\n",
        "        \"category\"   : str(row[\"Category\"]).strip(),\n",
        "        \"aspect\"     : str(row[\"Aspect\"]).strip(),\n",
        "        \"intent\"     : int(row[\"Intent\"]),\n",
        "        \"explanation\": str(row[\"Explanation\"]).strip(),\n",
        "        \"country\"    : str(row[\"Country\"]).strip(),\n",
        "        \"source\"     : str(row[\"Source\"]).strip(),\n",
        "        \"sector\"     : str(row[\"Sector\"]).strip(),\n",
        "    }, indent=2)\n",
        "\n",
        "\n",
        "def parse_json_response(text: str) -> dict:\n",
        "    try:\n",
        "        m = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if m:\n",
        "            return json.loads(m.group())\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 9 â”€â”€ Load Model + Tokenizer\n",
        "#\n",
        "#  8B model fits on T4 (16 GB) using 4-bit quantisation.\n",
        "#  NOTE: Unlike 38B, InternVL2.5-8B's ViT is small enough\n",
        "#        that 4-bit is safe â€” no accuracy degradation.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(f\"\\nLoading {MODEL_PATH} in 4-bit on T4...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    trust_remote_code = True,\n",
        "    use_fast          = False,\n",
        ")\n",
        "\n",
        "# 4-bit NF4 quantisation â€” fits 8B model in ~8 GB VRAM leaving room for training\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit               = True,\n",
        "    bnb_4bit_quant_type        = \"nf4\",\n",
        "    bnb_4bit_use_double_quant  = True,\n",
        "    bnb_4bit_compute_dtype     = torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    quantization_config = quant_config,\n",
        "    low_cpu_mem_usage   = True,\n",
        "    trust_remote_code   = True,\n",
        "    use_flash_attn      = False,   # flash-attn not compatible with 4-bit on T4\n",
        ").eval()\n",
        "\n",
        "print(\"âœ… Model loaded\")\n",
        "\n",
        "# Apply LoRA to language_model backbone only (vision encoder stays frozen)\n",
        "lora_config = LoraConfig(\n",
        "    task_type      = TaskType.CAUSAL_LM,\n",
        "    r              = LORA_RANK,\n",
        "    lora_alpha     = LORA_RANK * 2,\n",
        "    lora_dropout   = 0.05,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias           = \"none\",\n",
        ")\n",
        "model.language_model = get_peft_model(model.language_model, lora_config)\n",
        "model.language_model.print_trainable_parameters()\n",
        "print(\"âœ… LoRA applied\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 10 â”€â”€ Build Samples\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "train_df, eval_df = train_test_split(df, test_size=EVAL_SPLIT, random_state=RANDOM_SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "eval_df  = eval_df.reset_index(drop=True)\n",
        "print(f\"\\nSplit â†’ Train: {len(train_df)}  |  Eval: {len(eval_df)}\")\n",
        "\n",
        "\n",
        "def build_samples(dataframe, split_name=\"train\"):\n",
        "    samples, skipped = [], 0\n",
        "    for i, row in dataframe.iterrows():\n",
        "        uni_id     = str(row[\"uni_id\"]).strip()\n",
        "        video_path = find_video(uni_id)\n",
        "        frames     = extract_frames(video_path)\n",
        "\n",
        "        if not frames:\n",
        "            print(f\"  âš ï¸  [{split_name}] Skipping {uni_id} â€” video not found\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        samples.append({\n",
        "            \"pil_images\" : frames,\n",
        "            \"question\"   : build_question(len(frames)),\n",
        "            \"answer\"     : row_to_json(row),\n",
        "            \"uni_id\"     : uni_id,\n",
        "            \"target_json\": row_to_json(row),\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 30 == 0:\n",
        "            print(f\"  [{split_name}] {i+1}/{len(dataframe)}  skipped={skipped}\")\n",
        "\n",
        "    print(f\"  âœ… [{split_name}] ready={len(samples)}  skipped={skipped}\")\n",
        "    return samples\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“¦ Building training samples...\")\n",
        "train_samples = build_samples(train_df, \"train\")\n",
        "\n",
        "print(\"\\nğŸ“¦ Building eval samples...\")\n",
        "eval_samples  = build_samples(eval_df,  \"eval\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 11 â”€â”€ PyTorch Dataset + Collator\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "IMG_CONTEXT_TOKEN = '<IMG_CONTEXT>'\n",
        "IMG_TOKEN         = '<image>'\n",
        "\n",
        "class InternVL25Dataset(TorchDataset):\n",
        "    \"\"\"\n",
        "    Tokenises samples using InternVL2.5 chat template:\n",
        "      <|im_start|>system\\\\n{system}<|im_end|>\n",
        "      <|im_start|>user\\\\n{question with <IMG_CONTEXT> expanded}<|im_end|>\n",
        "      <|im_start|>assistant\\\\n{answer}<|im_end|>\n",
        "\n",
        "    Each <image> placeholder is expanded to num_image_token copies\n",
        "    of <IMG_CONTEXT> before tokenisation â€” this is mandatory for\n",
        "    InternVL2.5 to correctly align visual tokens with text tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, tokenizer, model, max_length=2048):\n",
        "        self.samples         = samples\n",
        "        self.tokenizer       = tokenizer\n",
        "        self.max_length      = max_length\n",
        "        self.num_image_token = model.num_image_token  # 256 for ViT-448\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s          = self.samples[idx]\n",
        "        pil_images = s[\"pil_images\"]\n",
        "        question   = s[\"question\"]\n",
        "        answer     = s[\"answer\"]\n",
        "\n",
        "        # Convert PIL frames â†’ pixel_values\n",
        "        pixel_values = pil_to_pixel_values(pil_images)  # (N, 3, 448, 448)\n",
        "\n",
        "        # Expand each <image> â†’ 256x <IMG_CONTEXT>\n",
        "        expanded_q = question.replace(\n",
        "            IMG_TOKEN,\n",
        "            IMG_CONTEXT_TOKEN * self.num_image_token\n",
        "        )\n",
        "\n",
        "        # Full conversation string in InternVL2.5 template format\n",
        "        full_text = (\n",
        "            f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\"\n",
        "            f\"<|im_start|>user\\n{expanded_q}<|im_end|>\"\n",
        "            f\"<|im_start|>assistant\\n{answer}<|im_end|>\"\n",
        "        )\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            return_tensors = \"pt\",\n",
        "            max_length     = self.max_length,\n",
        "            truncation     = True,\n",
        "            padding        = False,\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        # Build labels â€” mask user + system portion, only train on assistant answer\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Find where assistant response starts\n",
        "        assistant_header = self.tokenizer.encode(\n",
        "            \"<|im_start|>assistant\\n\", add_special_tokens=False\n",
        "        )\n",
        "        header_len = len(assistant_header)\n",
        "        mask_until = 0\n",
        "        for pos in range(len(input_ids) - header_len):\n",
        "            if input_ids[pos : pos + header_len].tolist() == assistant_header:\n",
        "                mask_until = pos + header_len   # train from answer onwards only\n",
        "\n",
        "        labels[:mask_until] = -100\n",
        "\n",
        "        # Mask padding tokens\n",
        "        pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\"     : input_ids,\n",
        "            \"attention_mask\": torch.ones_like(input_ids),\n",
        "            \"labels\"        : labels,\n",
        "            \"pixel_values\"  : pixel_values,   # (N_frames, 3, 448, 448)\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "\n",
        "    input_ids      = pad_sequence([b[\"input_ids\"]      for b in batch], batch_first=True, padding_value=pad_id)\n",
        "    attention_mask = pad_sequence([b[\"attention_mask\"] for b in batch], batch_first=True, padding_value=0)\n",
        "    labels         = pad_sequence([b[\"labels\"]         for b in batch], batch_first=True, padding_value=-100)\n",
        "\n",
        "    # Concatenate all frames across the batch\n",
        "    pixel_values = torch.cat([b[\"pixel_values\"] for b in batch], dim=0)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\"     : input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\"        : labels,\n",
        "        \"pixel_values\"  : pixel_values,\n",
        "    }\n",
        "\n",
        "\n",
        "train_torch_ds = InternVL25Dataset(train_samples, tokenizer, model, max_length=MAX_SEQ_LEN)\n",
        "print(f\"âœ… PyTorch dataset ready: {len(train_torch_ds)} training samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 12 â”€â”€ Train\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "model.train()\n",
        "model.language_model.enable_input_require_grads()   # needed for LoRA + gradient checkpointing\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                    = OUTPUT_DIR,\n",
        "    per_device_train_batch_size   = BATCH_SIZE,\n",
        "    gradient_accumulation_steps   = GRAD_ACCUM,\n",
        "    num_train_epochs              = EPOCHS,\n",
        "    learning_rate                 = 2e-4,\n",
        "    bf16                          = True,\n",
        "    logging_steps                 = 5,\n",
        "    save_steps                    = 100,\n",
        "    save_total_limit              = 2,\n",
        "    warmup_ratio                  = 0.03,\n",
        "    lr_scheduler_type             = \"cosine\",\n",
        "    optim                         = \"paged_adamw_8bit\",   # saves VRAM on T4\n",
        "    seed                          = RANDOM_SEED,\n",
        "    remove_unused_columns         = False,\n",
        "    dataloader_pin_memory         = False,\n",
        "    gradient_checkpointing        = True,\n",
        "    gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
        "    report_to                     = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_torch_ds,\n",
        "    data_collator = collate_fn,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ Training started...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.language_model.save_pretrained(\"internvl25_8b_lora_adapter\")\n",
        "tokenizer.save_pretrained(\"internvl25_8b_lora_adapter\")\n",
        "os.makedirs(ADAPTER_DRIVE, exist_ok=True)\n",
        "shutil.copytree(\"internvl25_8b_lora_adapter\", ADAPTER_DRIVE, dirs_exist_ok=True)\n",
        "print(f\"âœ… Adapter saved â†’ {ADAPTER_DRIVE}\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 13 â”€â”€ Inference Function\n",
        "#\n",
        "#  model.chat() is InternVL's official inference API.\n",
        "#  It handles <image> expansion + generation internally.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "model.eval()\n",
        "\n",
        "GENERATION_CONFIG = dict(\n",
        "    max_new_tokens = 512,\n",
        "    do_sample      = False,\n",
        "    temperature    = 0.1,\n",
        "    num_beams      = 1,\n",
        ")\n",
        "\n",
        "def predict(pil_images: list) -> dict:\n",
        "    \"\"\"\n",
        "    Run inference on a list of PIL frames using model.chat().\n",
        "    pixel_values must be cast to bfloat16 to match model dtype.\n",
        "    \"\"\"\n",
        "    pixel_values = pil_to_pixel_values(pil_images).to(\n",
        "        dtype  = torch.bfloat16,\n",
        "        device = model.device,\n",
        "    )\n",
        "    question = build_question(len(pil_images))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        response = model.chat(\n",
        "            tokenizer         = tokenizer,\n",
        "            pixel_values      = pixel_values,\n",
        "            question          = question,\n",
        "            generation_config = GENERATION_CONFIG,\n",
        "        )\n",
        "\n",
        "    return parse_json_response(response)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 14 â”€â”€ Run Inference on ALL samples (train + eval)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"  ğŸ” Running inference on ALL samples (train + eval)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "all_samples = train_samples + eval_samples\n",
        "all_results = []\n",
        "\n",
        "for i, s in enumerate(all_samples):\n",
        "    pred = predict(s[\"pil_images\"])\n",
        "    tgt  = json.loads(s[\"target_json\"])\n",
        "\n",
        "    all_results.append({\n",
        "        \"uni_id\"          : s[\"uni_id\"],\n",
        "        \"split\"           : \"train\" if i < len(train_samples) else \"eval\",\n",
        "        \"true_category\"   : tgt.get(\"category\",    \"\"),\n",
        "        \"true_aspect\"     : tgt.get(\"aspect\",       \"\"),\n",
        "        \"true_intent\"     : str(tgt.get(\"intent\",   \"\")),\n",
        "        \"true_explanation\": tgt.get(\"explanation\",  \"\"),\n",
        "        \"true_country\"    : tgt.get(\"country\",      \"\"),\n",
        "        \"true_source\"     : tgt.get(\"source\",       \"\"),\n",
        "        \"true_sector\"     : tgt.get(\"sector\",       \"\"),\n",
        "        \"pred_category\"   : str(pred.get(\"category\",    \"\")).strip(),\n",
        "        \"pred_aspect\"     : str(pred.get(\"aspect\",       \"\")).strip(),\n",
        "        \"pred_intent\"     : str(pred.get(\"intent\",       \"\")).strip(),\n",
        "        \"pred_explanation\": str(pred.get(\"explanation\",  \"\")).strip(),\n",
        "        \"pred_country\"    : str(pred.get(\"country\",      \"\")).strip(),\n",
        "        \"pred_source\"     : str(pred.get(\"source\",       \"\")).strip(),\n",
        "        \"pred_sector\"     : str(pred.get(\"sector\",       \"\")).strip(),\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 20 == 0:\n",
        "        print(f\"  Inferred {i+1}/{len(all_samples)}\")\n",
        "\n",
        "print(f\"âœ… Inference done for {len(all_results)} samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 15 â”€â”€ Accuracy Scoring\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "scorer = rs.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def score_split(rows, label=\"ALL\"):\n",
        "    if not rows:\n",
        "        return {}\n",
        "\n",
        "    def acc(t, p):\n",
        "        return accuracy_score(\n",
        "            [x.lower().strip() for x in t],\n",
        "            [x.lower().strip() for x in p]\n",
        "        )\n",
        "\n",
        "    rouge_scores = [\n",
        "        scorer.score(r[\"true_explanation\"], r[\"pred_explanation\"])[\"rougeL\"].fmeasure\n",
        "        for r in rows\n",
        "    ]\n",
        "    avg_rouge = float(np.mean(rouge_scores))\n",
        "\n",
        "    cat_acc = acc([r[\"true_category\"] for r in rows], [r[\"pred_category\"] for r in rows])\n",
        "    asp_acc = acc([r[\"true_aspect\"]   for r in rows], [r[\"pred_aspect\"]   for r in rows])\n",
        "    int_acc = acc([r[\"true_intent\"]   for r in rows], [r[\"pred_intent\"]   for r in rows])\n",
        "    cty_acc = acc([r[\"true_country\"]  for r in rows], [r[\"pred_country\"]  for r in rows])\n",
        "    sec_acc = acc([r[\"true_sector\"]   for r in rows], [r[\"pred_sector\"]   for r in rows])\n",
        "    src_acc = acc([r[\"true_source\"]   for r in rows], [r[\"pred_source\"]   for r in rows])\n",
        "\n",
        "    overall = (cat_acc*0.25 + asp_acc*0.25 + int_acc*0.15 +\n",
        "               avg_rouge*0.20 + sec_acc*0.10 + cty_acc*0.03 + src_acc*0.02)\n",
        "\n",
        "    return {\n",
        "        \"split\"             : label,\n",
        "        \"n_samples\"         : len(rows),\n",
        "        \"category_acc\"      : round(cat_acc   * 100, 2),\n",
        "        \"aspect_acc\"        : round(asp_acc   * 100, 2),\n",
        "        \"intent_acc\"        : round(int_acc   * 100, 2),\n",
        "        \"country_acc\"       : round(cty_acc   * 100, 2),\n",
        "        \"source_acc\"        : round(src_acc   * 100, 2),\n",
        "        \"sector_acc\"        : round(sec_acc   * 100, 2),\n",
        "        \"explanation_rougeL\": round(avg_rouge * 100, 2),\n",
        "        \"OVERALL_SCORE_%\"   : round(overall   * 100, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "train_rows   = [r for r in all_results if r[\"split\"] == \"train\"]\n",
        "eval_rows    = [r for r in all_results if r[\"split\"] == \"eval\"]\n",
        "train_scores = score_split(train_rows,  \"TRAIN\")\n",
        "eval_scores  = score_split(eval_rows,   \"EVAL\")\n",
        "all_scores   = score_split(all_results, \"ALL\")\n",
        "\n",
        "def print_scores(s: dict):\n",
        "    print(f\"\\n  â”€â”€ {s['split']} SET  ({s['n_samples']} samples) â”€â”€\")\n",
        "    print(f\"   Category    Accuracy  : {s['category_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Aspect      Accuracy  : {s['aspect_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Intent      Accuracy  : {s['intent_acc']:>7.2f} %   (weight 15%)\")\n",
        "    print(f\"   Explanation ROUGE-L   : {s['explanation_rougeL']:>7.2f} %   (weight 20%)\")\n",
        "    print(f\"   Sector      Accuracy  : {s['sector_acc']:>7.2f} %   (weight 10%)\")\n",
        "    print(f\"   Country     Accuracy  : {s['country_acc']:>7.2f} %   (weight  3%)\")\n",
        "    print(f\"   Source      Accuracy  : {s['source_acc']:>7.2f} %   (weight  2%)\")\n",
        "    print(f\"   {'â”€'*42}\")\n",
        "    print(f\"   â­ OVERALL SCORE      : {s['OVERALL_SCORE_%']:>7.2f} %\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"              ğŸ“Š ACCURACY REPORT\")\n",
        "print(\"=\"*65)\n",
        "print_scores(train_scores)\n",
        "print_scores(eval_scores)\n",
        "print_scores(all_scores)\n",
        "print(\"=\"*65)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 16 â”€â”€ Save Results to Excel (7 sheets)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "results_df = pd.DataFrame(all_results)\n",
        "summary_df = pd.DataFrame([train_scores, eval_scores, all_scores])\n",
        "\n",
        "os.makedirs(os.path.dirname(RESULTS_EXCEL_DRIVE), exist_ok=True)\n",
        "\n",
        "with pd.ExcelWriter(RESULTS_EXCEL_DRIVE, engine=\"openpyxl\") as writer:\n",
        "    summary_df.to_excel(writer, sheet_name=\"Accuracy Summary\", index=False)\n",
        "    results_df.to_excel(writer, sheet_name=\"Per Sample\",       index=False)\n",
        "\n",
        "    for field in [\"category\", \"aspect\", \"intent\", \"sector\", \"country\", \"source\"]:\n",
        "        breakdown = results_df[[\n",
        "            \"uni_id\", \"split\",\n",
        "            f\"true_{field}\", f\"pred_{field}\"\n",
        "        ]].copy()\n",
        "        breakdown[\"correct\"] = (\n",
        "            breakdown[f\"true_{field}\"].str.lower().str.strip() ==\n",
        "            breakdown[f\"pred_{field}\"].str.lower().str.strip()\n",
        "        )\n",
        "        breakdown.to_excel(writer, sheet_name=f\"{field.capitalize()} Detail\", index=False)\n",
        "\n",
        "print(f\"\\nâœ… Results saved â†’ {RESULTS_EXCEL_DRIVE}\")\n",
        "print(\"\\nğŸ‰ ALL DONE!\")"
      ]
    }
  ]
}