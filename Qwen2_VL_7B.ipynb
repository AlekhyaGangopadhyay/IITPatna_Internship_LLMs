{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0Gufv4fm/Xu1SGkRootfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekhyaGangopadhyay/IITPatna_Internship_LLMs/blob/main/Qwen2_VL_7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmpV3mT8madE"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  Qwen2-VL-7B  â–¸  Finance Harassment/Abuse Video Dataset\n",
        "#  Full Pipeline: Train â†’ Infer â†’ Accuracy Score\n",
        "#  Dataset : iamalekhya/Finance_set  (318 videos, 9 columns)\n",
        "#  Run on  : Google Colab  â†’  Runtime â–¸ T4 / A100 GPU\n",
        "# ============================================================\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â”€â”€ Install (run once, then restart runtime)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â€” Run this, then Runtime > Restart Session, then run from Cell 2\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install rouge-score qwen-vl-utils openpyxl opencv-python scikit-learn\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 2 â”€â”€ Mount Google Drive\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 3 â”€â”€ âš™ï¸  CONFIG  â€” only edit this section\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "# â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "HF_DATASET     = \"iamalekhya/Finance_set\"\n",
        "EXCEL_FALLBACK = \"/content/drive/MyDrive/IITP_Internship/Finance_set.xlsx\"\n",
        "\n",
        "# â”€â”€ BUG FIX 1: VIDEO_DIR must be a LIST, not a tuple of strings â”€â”€\n",
        "# Your original code had:\n",
        "#   VIDEO_DIR = \"path1\",\"path2\",\"path3\"   â† Python makes this a TUPLE, os.path.join() crashes\n",
        "# Fixed: proper Python list\n",
        "VIDEO_DIRS = [\n",
        "    \"/content/drive/My Drive/IITP_Internship/AG\",\n",
        "    \"/content/drive/My Drive/IITP_Internship/ag\",\n",
        "    \"/content/drive/My Drive/IITP_Internship/Alekhya\",\n",
        "    \"/content/drive/My Drive/IITP_Internship/Pritam\",\n",
        "    \"/content/drive/My Drive/IITP_Internship/Videos\",\n",
        "]\n",
        "# NOTE: \"Shared with me\" drives mount as \"Shared drives\" in Colab.\n",
        "# If videos still not found, run this in a cell to check exact path:\n",
        "#   import os; print(os.listdir('/content/drive/Shared drives/'))\n",
        "VIDEO_EXT = \".mp4\"\n",
        "\n",
        "# â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "NUM_FRAMES  = 8\n",
        "MAX_SEQ_LEN = 2048\n",
        "EPOCHS      = 3\n",
        "BATCH_SIZE  = 1\n",
        "GRAD_ACCUM  = 8\n",
        "LORA_RANK   = 16\n",
        "EVAL_SPLIT  = 0.10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "OUTPUT_DIR          = \"./qwen2vl_finance\"\n",
        "ADAPTER_DRIVE       = \"/content/drive/MyDrive/IIT_Internship/qwen2vl_finance_adapter\"\n",
        "RESULTS_EXCEL_DRIVE = \"/content/drive/MyDrive/IIT_Internship/finance_eval_results.xlsx\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 4 â”€â”€ Imports\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "import os, re, json, cv2, shutil, torch, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from datasets import Dataset, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rouge_score import rouge_scorer as rs\n",
        "from unsloth import FastVisionModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"âœ… Imports OK\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 5 â”€â”€ Load Dataset\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    ds = load_dataset(HF_DATASET, split=\"train\")\n",
        "    df = ds.to_pandas()\n",
        "    print(f\"âœ… Loaded from HuggingFace: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  HF load failed ({e}), falling back to Excel...\")\n",
        "    df = pd.read_excel(EXCEL_FALLBACK)\n",
        "    print(f\"âœ… Loaded from Excel: {len(df)} rows\")\n",
        "\n",
        "# Normalise column names\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "print(\"   Columns:\", df.columns.tolist())\n",
        "\n",
        "REQUIRED = [\"uni_id\", \"Category\", \"Aspect\", \"Intent\", \"Explanation\", \"Country\", \"Source\", \"Sector\"]\n",
        "missing  = [c for c in REQUIRED if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}  â€” actual: {df.columns.tolist()}\")\n",
        "\n",
        "df = df.dropna(subset=REQUIRED).reset_index(drop=True)\n",
        "print(f\"   Clean rows : {len(df)}\")\n",
        "print(f\"   Categories : {df['Category'].unique().tolist()}\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 6 â”€â”€ Prompt Templates\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "ALL_ASPECTS = sorted(df[\"Aspect\"].unique().tolist())\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are an expert video analyst specialising in financial harassment, fraud, abuse, and workplace misconduct.\n",
        "\n",
        "Given video frames, output a single valid JSON object with EXACTLY these fields:\n",
        "\n",
        "{{\n",
        "  \"category\"   : \"Harassment\" | \"Bullying\",\n",
        "  \"aspect\"     : one of the known aspect labels (see list below),\n",
        "  \"intent\"     : 1 (intentional) | 0 (unintentional),\n",
        "  \"explanation\": \"1-2 sentence description of what is happening and why it is problematic\",\n",
        "  \"country\"    : \"country name or Unknown\",\n",
        "  \"source\"     : \"YouTube | News | Training | Documentary | Other\",\n",
        "  \"sector\"     : \"Finance | Healthcare | Education | Retail | Tech | Government | General\"\n",
        "}}\n",
        "\n",
        "Known aspect labels (pick the closest match):\n",
        "{json.dumps(ALL_ASPECTS, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the JSON. No preamble, no markdown, no extra text.\n",
        "- Use double quotes for all strings.\n",
        "- intent must be an integer (1 or 0), not a string.\"\"\"\n",
        "\n",
        "USER_PROMPT = \"\"\"These frames are sampled evenly from a video about financial misconduct or harassment.\n",
        "\n",
        "Carefully observe:\n",
        "â€¢ Who is involved and what is happening\n",
        "â€¢ The type and form of misconduct\n",
        "â€¢ Whether the act is intentional\n",
        "â€¢ The country/industry context\n",
        "\n",
        "Return your analysis as a JSON object only.\"\"\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 â”€â”€ Helpers\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "def find_video(uni_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Search across all VIDEO_DIRS for a file named uni_id.mp4 (case-insensitive).\n",
        "    Returns full path if found, else empty string.\n",
        "    \"\"\"\n",
        "    filename = f\"{uni_id}{VIDEO_EXT}\"\n",
        "    for folder in VIDEO_DIRS:\n",
        "        path = os.path.join(folder, filename)\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "    # Try case-insensitive search in each folder\n",
        "    for folder in VIDEO_DIRS:\n",
        "        if not os.path.isdir(folder):\n",
        "            continue\n",
        "        for f in os.listdir(folder):\n",
        "            if f.lower() == filename.lower():\n",
        "                return os.path.join(folder, f)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_frames(video_path: str, n: int = NUM_FRAMES) -> list:\n",
        "    \"\"\"Return n evenly-spaced PIL frames from a video file.\"\"\"\n",
        "    if not video_path or not os.path.exists(video_path):\n",
        "        return []\n",
        "    cap   = cv2.VideoCapture(video_path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    frames = []\n",
        "    for idx in np.linspace(0, total - 1, n, dtype=int):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, frame = cap.read()\n",
        "        if ok:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def row_to_json(row) -> str:\n",
        "    return json.dumps({\n",
        "        \"category\"   : str(row[\"Category\"]).strip(),\n",
        "        \"aspect\"     : str(row[\"Aspect\"]).strip(),\n",
        "        \"intent\"     : int(row[\"Intent\"]),\n",
        "        \"explanation\": str(row[\"Explanation\"]).strip(),\n",
        "        \"country\"    : str(row[\"Country\"]).strip(),\n",
        "        \"source\"     : str(row[\"Source\"]).strip(),\n",
        "        \"sector\"     : str(row[\"Sector\"]).strip(),\n",
        "    }, indent=2)\n",
        "\n",
        "\n",
        "def parse_json_response(text: str) -> dict:\n",
        "    try:\n",
        "        m = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if m:\n",
        "            return json.loads(m.group())\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 8 â”€â”€ Load Model + LoRA\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\nLoading Qwen2-VL-7B...\")\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name                 = \"unsloth/Qwen2-VL-7B-Instruct\",\n",
        "    load_in_4bit               = True,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r            = LORA_RANK,\n",
        "    lora_alpha   = LORA_RANK,\n",
        "    lora_dropout = 0,\n",
        "    bias         = \"none\",\n",
        "    random_state = RANDOM_SEED,\n",
        ")\n",
        "print(\"âœ… Model + LoRA ready\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# REPLACE YOUR CELL 9 + CELL 10 WITH THIS\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "# â”€â”€ CELL 9 â”€â”€ Build Train / Eval splits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "train_df, eval_df = train_test_split(df, test_size=EVAL_SPLIT, random_state=RANDOM_SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "eval_df  = eval_df.reset_index(drop=True)\n",
        "print(f\"\\nSplit â†’ Train: {len(train_df)}  |  Eval: {len(eval_df)}\")\n",
        "\n",
        "\n",
        "def build_samples(dataframe, split_name=\"train\"):\n",
        "    \"\"\"Build raw samples list â€” text + PIL images kept together.\"\"\"\n",
        "    samples, skipped = [], 0\n",
        "\n",
        "    for i, row in dataframe.iterrows():\n",
        "        uni_id     = str(row[\"uni_id\"]).strip()\n",
        "        video_path = find_video(uni_id)\n",
        "        frames     = extract_frames(video_path)\n",
        "\n",
        "        if not frames:\n",
        "            print(f\"  âš ï¸  [{split_name}] Skipping {uni_id} â€” video not found\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        target = row_to_json(row)\n",
        "\n",
        "        user_content = [{\"type\": \"image\", \"image\": f} for f in frames]\n",
        "        user_content.append({\"type\": \"text\", \"text\": USER_PROMPT})\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\",    \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
        "            {\"role\": \"user\",      \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": target}]},\n",
        "        ]\n",
        "\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "\n",
        "        samples.append({\n",
        "            \"text\"       : text,\n",
        "            \"images\"     : frames,\n",
        "            \"uni_id\"     : uni_id,\n",
        "            \"target_json\": target,\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 30 == 0:\n",
        "            print(f\"  [{split_name}] {i+1}/{len(dataframe)}  skipped={skipped}\")\n",
        "\n",
        "    print(f\"  âœ… [{split_name}] ready={len(samples)}  skipped={skipped}\")\n",
        "    return samples\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“¦ Building training samples...\")\n",
        "train_samples = build_samples(train_df, \"train\")\n",
        "\n",
        "print(\"\\nğŸ“¦ Building eval samples...\")\n",
        "eval_samples  = build_samples(eval_df,  \"eval\")\n",
        "\n",
        "print(f\"\\nâœ… Total usable â†’ Train: {len(train_samples)}  Eval: {len(eval_samples)}\")\n",
        "\n",
        "\n",
        "# â”€â”€ CELL 10 â”€â”€ Custom Collator + Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class VideoQADataset(TorchDataset):\n",
        "    \"\"\"\n",
        "    Tokenises each sample on-the-fly, passing PIL images to the\n",
        "    processor so pixel_values are created correctly.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, tokenizer, max_length=2048):\n",
        "        self.samples    = samples\n",
        "        self.tokenizer  = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s      = self.samples[idx]\n",
        "        text   = s[\"text\"]\n",
        "        images = s[\"images\"]   # list of PIL Images\n",
        "\n",
        "        # Tokenise text + images together\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            images          = images,\n",
        "            return_tensors  = \"pt\",\n",
        "            max_length      = self.max_length,\n",
        "            truncation      = True,\n",
        "            padding         = False,\n",
        "        )\n",
        "\n",
        "        # Squeeze batch dim added by return_tensors=\"pt\"\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "        # Labels = input_ids (causal LM); mask padding with -100\n",
        "        labels = item[\"input_ids\"].clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "        item[\"labels\"] = labels\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pad a batch of variable-length tokenised samples.\"\"\"\n",
        "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "\n",
        "    input_ids      = pad_sequence([b[\"input_ids\"]      for b in batch], batch_first=True, padding_value=pad_id)\n",
        "    attention_mask = pad_sequence([b[\"attention_mask\"] for b in batch], batch_first=True, padding_value=0)\n",
        "    labels         = pad_sequence([b[\"labels\"]         for b in batch], batch_first=True, padding_value=-100)\n",
        "\n",
        "    result = {\n",
        "        \"input_ids\"     : input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\"        : labels,\n",
        "    }\n",
        "\n",
        "    # pixel_values may or may not be present depending on processor version\n",
        "    if \"pixel_values\" in batch[0]:\n",
        "        result[\"pixel_values\"] = torch.cat([b[\"pixel_values\"].unsqueeze(0) for b in batch], dim=0)\n",
        "\n",
        "    # image_grid_thw is used by Qwen2-VL to know frame layout\n",
        "    if \"image_grid_thw\" in batch[0]:\n",
        "        result[\"image_grid_thw\"] = torch.cat([b[\"image_grid_thw\"] for b in batch], dim=0)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Build PyTorch datasets\n",
        "train_torch_dataset = VideoQADataset(train_samples, tokenizer, max_length=MAX_SEQ_LEN)\n",
        "\n",
        "FastVisionModel.for_training(model)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                  = OUTPUT_DIR,\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    gradient_accumulation_steps = GRAD_ACCUM,\n",
        "    num_train_epochs            = EPOCHS,\n",
        "    learning_rate               = 2e-4,\n",
        "    fp16                        = not torch.cuda.is_bf16_supported(),\n",
        "    bf16                        = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps               = 5,\n",
        "    save_steps                  = 100,\n",
        "    save_total_limit            = 2,\n",
        "    warmup_ratio                = 0.03,\n",
        "    lr_scheduler_type           = \"cosine\",\n",
        "    optim                       = \"adamw_8bit\",\n",
        "    seed                        = RANDOM_SEED,\n",
        "    dataloader_pin_memory       = False,   # avoids CUDA pin_memory issues with PIL\n",
        "    remove_unused_columns       = False,\n",
        "    report_to                   = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_torch_dataset,\n",
        "    data_collator = collate_fn,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ Training started...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# Save adapter\n",
        "import shutil, os\n",
        "model.save_pretrained(\"qwen2vl_finance_adapter\")\n",
        "tokenizer.save_pretrained(\"qwen2vl_finance_adapter\")\n",
        "os.makedirs(ADAPTER_DRIVE, exist_ok=True)\n",
        "shutil.copytree(\"qwen2vl_finance_adapter\", ADAPTER_DRIVE, dirs_exist_ok=True)\n",
        "print(f\"âœ… Adapter saved â†’ {ADAPTER_DRIVE}\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 11 â”€â”€ Inference helper\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "def predict(frames: list) -> dict:\n",
        "    \"\"\"Run model on a list of PIL frames; return parsed dict.\"\"\"\n",
        "    # BUG FIX 2: type must be \"image\" not \"video\" â€” Qwen2-VL takes individual frames as images\n",
        "    user_content = [{\"type\": \"image\", \"image\": f} for f in frames]\n",
        "    user_content.append({\"type\": \"text\", \"text\": USER_PROMPT})\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
        "        {\"role\": \"user\",   \"content\": user_content},\n",
        "    ]\n",
        "\n",
        "    text_in = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer(text_in, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens = 512,\n",
        "            temperature    = 0.1,\n",
        "            do_sample      = False,\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    return parse_json_response(decoded)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 12 â”€â”€ Run inference on FULL dataset (train + eval)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"  ğŸ” Running inference on ALL samples (train + eval)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "all_samples = train_samples + eval_samples\n",
        "all_results = []\n",
        "\n",
        "for i, s in enumerate(all_samples):\n",
        "    # BUG FIX 3: s[\"videos\"] â†’ s[\"images\"]  (key was named \"images\" when built above)\n",
        "    pred = predict(s[\"images\"])\n",
        "    tgt  = json.loads(s[\"target_json\"])\n",
        "\n",
        "    all_results.append({\n",
        "        \"uni_id\"          : s[\"uni_id\"],\n",
        "        \"split\"           : \"train\" if i < len(train_samples) else \"eval\",\n",
        "        # ground truth\n",
        "        \"true_category\"   : tgt.get(\"category\",    \"\"),\n",
        "        \"true_aspect\"     : tgt.get(\"aspect\",       \"\"),\n",
        "        \"true_intent\"     : str(tgt.get(\"intent\",   \"\")),\n",
        "        \"true_explanation\": tgt.get(\"explanation\",  \"\"),\n",
        "        \"true_country\"    : tgt.get(\"country\",      \"\"),\n",
        "        \"true_source\"     : tgt.get(\"source\",       \"\"),\n",
        "        \"true_sector\"     : tgt.get(\"sector\",       \"\"),\n",
        "        # predictions\n",
        "        \"pred_category\"   : str(pred.get(\"category\",    \"\")).strip(),\n",
        "        \"pred_aspect\"     : str(pred.get(\"aspect\",       \"\")).strip(),\n",
        "        \"pred_intent\"     : str(pred.get(\"intent\",       \"\")).strip(),\n",
        "        \"pred_explanation\": str(pred.get(\"explanation\",  \"\")).strip(),\n",
        "        \"pred_country\"    : str(pred.get(\"country\",      \"\")).strip(),\n",
        "        \"pred_source\"     : str(pred.get(\"source\",       \"\")).strip(),\n",
        "        \"pred_sector\"     : str(pred.get(\"sector\",       \"\")).strip(),\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 20 == 0:\n",
        "        print(f\"  Inferred {i+1}/{len(all_samples)}\")\n",
        "\n",
        "print(f\"âœ… Inference done for {len(all_results)} samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 13 â”€â”€ Accuracy Scoring\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "scorer = rs.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def score_split(rows, label=\"ALL\"):\n",
        "    if not rows:\n",
        "        return {}\n",
        "\n",
        "    def acc(t, p):\n",
        "        return accuracy_score(\n",
        "            [x.lower().strip() for x in t],\n",
        "            [x.lower().strip() for x in p]\n",
        "        )\n",
        "\n",
        "    tc  = [r[\"true_category\"]    for r in rows]\n",
        "    pc  = [r[\"pred_category\"]    for r in rows]\n",
        "    ta  = [r[\"true_aspect\"]      for r in rows]\n",
        "    pa  = [r[\"pred_aspect\"]      for r in rows]\n",
        "    ti  = [r[\"true_intent\"]      for r in rows]\n",
        "    pi  = [r[\"pred_intent\"]      for r in rows]\n",
        "    tn  = [r[\"true_country\"]     for r in rows]\n",
        "    pn  = [r[\"pred_country\"]     for r in rows]\n",
        "    ts  = [r[\"true_sector\"]      for r in rows]\n",
        "    ps  = [r[\"pred_sector\"]      for r in rows]\n",
        "    tsr = [r[\"true_source\"]      for r in rows]\n",
        "    psr = [r[\"pred_source\"]      for r in rows]\n",
        "\n",
        "    rouge_scores = [\n",
        "        scorer.score(r[\"true_explanation\"], r[\"pred_explanation\"])[\"rougeL\"].fmeasure\n",
        "        for r in rows\n",
        "    ]\n",
        "    avg_rouge = float(np.mean(rouge_scores))\n",
        "\n",
        "    cat_acc = acc(tc, pc)\n",
        "    asp_acc = acc(ta, pa)\n",
        "    int_acc = acc(ti, pi)\n",
        "    cty_acc = acc(tn, pn)\n",
        "    sec_acc = acc(ts, ps)\n",
        "    src_acc = acc(tsr, psr)\n",
        "\n",
        "    overall = (\n",
        "        cat_acc   * 0.25 +\n",
        "        asp_acc   * 0.25 +\n",
        "        int_acc   * 0.15 +\n",
        "        avg_rouge * 0.20 +\n",
        "        sec_acc   * 0.10 +\n",
        "        cty_acc   * 0.03 +\n",
        "        src_acc   * 0.02\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"split\"             : label,\n",
        "        \"n_samples\"         : len(rows),\n",
        "        \"category_acc\"      : round(cat_acc   * 100, 2),\n",
        "        \"aspect_acc\"        : round(asp_acc   * 100, 2),\n",
        "        \"intent_acc\"        : round(int_acc   * 100, 2),\n",
        "        \"country_acc\"       : round(cty_acc   * 100, 2),\n",
        "        \"source_acc\"        : round(src_acc   * 100, 2),\n",
        "        \"sector_acc\"        : round(sec_acc   * 100, 2),\n",
        "        \"explanation_rougeL\": round(avg_rouge * 100, 2),\n",
        "        \"OVERALL_SCORE_%\"   : round(overall   * 100, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "train_rows = [r for r in all_results if r[\"split\"] == \"train\"]\n",
        "eval_rows  = [r for r in all_results if r[\"split\"] == \"eval\"]\n",
        "\n",
        "train_scores = score_split(train_rows,  \"TRAIN\")\n",
        "eval_scores  = score_split(eval_rows,   \"EVAL\")\n",
        "all_scores   = score_split(all_results, \"ALL\")\n",
        "\n",
        "def print_scores(s: dict):\n",
        "    print(f\"\\n  â”€â”€ {s['split']} SET  ({s['n_samples']} samples) â”€â”€\")\n",
        "    print(f\"   Category    Accuracy  : {s['category_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Aspect      Accuracy  : {s['aspect_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Intent      Accuracy  : {s['intent_acc']:>7.2f} %   (weight 15%)\")\n",
        "    print(f\"   Explanation ROUGE-L   : {s['explanation_rougeL']:>7.2f} %   (weight 20%)\")\n",
        "    print(f\"   Sector      Accuracy  : {s['sector_acc']:>7.2f} %   (weight 10%)\")\n",
        "    print(f\"   Country     Accuracy  : {s['country_acc']:>7.2f} %   (weight  3%)\")\n",
        "    print(f\"   Source      Accuracy  : {s['source_acc']:>7.2f} %   (weight  2%)\")\n",
        "    print(f\"   {'â”€'*42}\")\n",
        "    print(f\"   â­ OVERALL SCORE      : {s['OVERALL_SCORE_%']:>7.2f} %\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"              ğŸ“Š ACCURACY REPORT\")\n",
        "print(\"=\"*65)\n",
        "print_scores(train_scores)\n",
        "print_scores(eval_scores)\n",
        "print_scores(all_scores)\n",
        "print(\"=\"*65)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 14 â”€â”€ Save everything to Excel in Drive\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "results_df = pd.DataFrame(all_results)\n",
        "summary_df = pd.DataFrame([train_scores, eval_scores, all_scores])\n",
        "\n",
        "os.makedirs(os.path.dirname(RESULTS_EXCEL_DRIVE), exist_ok=True)\n",
        "\n",
        "with pd.ExcelWriter(RESULTS_EXCEL_DRIVE, engine=\"openpyxl\") as writer:\n",
        "    summary_df.to_excel(writer, sheet_name=\"Accuracy Summary\", index=False)\n",
        "    results_df.to_excel(writer, sheet_name=\"Per Sample\",       index=False)\n",
        "\n",
        "    for field in [\"category\", \"aspect\", \"intent\", \"sector\", \"country\", \"source\"]:\n",
        "        breakdown = results_df[[\n",
        "            \"uni_id\", \"split\",\n",
        "            f\"true_{field}\", f\"pred_{field}\"\n",
        "        ]].copy()\n",
        "        breakdown[\"correct\"] = (\n",
        "            breakdown[f\"true_{field}\"].str.lower().str.strip() ==\n",
        "            breakdown[f\"pred_{field}\"].str.lower().str.strip()\n",
        "        )\n",
        "        breakdown.to_excel(writer, sheet_name=f\"{field.capitalize()} Detail\", index=False)\n",
        "\n",
        "print(f\"\\nâœ… Full results saved â†’ {RESULTS_EXCEL_DRIVE}\")\n",
        "print(\"\\nğŸ‰ ALL DONE!\")\n"
      ]
    }
  ]
}