{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQIp3jaI5T8S7AQELpc6PI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekhyaGangopadhyay/IITPatna_Internship_LLMs/blob/main/Aria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU2AK2zw5Nzj"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  Aria (rhymes-ai/Aria)  â–¸  Finance Harassment/Abuse Dataset\n",
        "#  Full Pipeline : Train (LoRA) â†’ Infer â†’ Accuracy Score\n",
        "#  Dataset       : iamalekhya/Finance_set  (318 videos, 9 cols)\n",
        "#\n",
        "#  Aria specs:\n",
        "#    25.3B total params | 3.9B activated (MoE) per token\n",
        "#    Multimodal-native MoE â€” superior video understanding\n",
        "#    64K token context window\n",
        "#\n",
        "#  GPU Requirements:\n",
        "#    bfloat16 full  â†’ ~50 GB  â†’ A100 80GB  (Colab Pro+)\n",
        "#    8-bit quant    â†’ ~26 GB  â†’ A100 40GB  (Colab Pro)  âœ… recommended\n",
        "#    4-bit quant    â†’ ~14 GB  â†’ T4 16GB    (Free Colab) âœ… fallback\n",
        "# ============================================================\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â”€â”€ Install  (run once â†’ Runtime â–¸ Restart session)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "!pip install \"transformers>=4.48.0\" accelerate sentencepiece peft bitsandbytes\n",
        "!pip install rouge-score openpyxl scikit-learn opencv-python\n",
        "!pip install flash-attn --no-build-isolation          # recommended for A100\n",
        "!pip install grouped_gemm==0.1.6                      # optional, faster MoE routing\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 2 â”€â”€ Mount Google Drive\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 3 â”€â”€ âš™ï¸  CONFIG  â† only edit this section\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "MODEL_PATH = \"rhymes-ai/Aria\"\n",
        "\n",
        "# Quantisation â€” choose based on your GPU:\n",
        "#   \"none\"  â†’ bfloat16 full precision  (A100 80GB)\n",
        "#   \"8bit\"  â†’ 8-bit bitsandbytes       (A100 40GB) â† recommended\n",
        "#   \"4bit\"  â†’ 4-bit NF4                (T4 16GB free Colab)\n",
        "QUANTISATION = \"4bit\"\n",
        "\n",
        "# Dataset\n",
        "HF_DATASET     = \"iamalekhya/Finance_set\"\n",
        "EXCEL_FALLBACK = \"/content/drive/MyDrive/IITP_Internship/Finance_set.xlsx\"\n",
        "\n",
        "# Videos in Google Drive (all folders searched automatically)\n",
        "VIDEO_DIRS = [\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/AG\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/ag\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Alekhya\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Pritam\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Videos\",\n",
        "]\n",
        "VIDEO_EXT = \".mp4\"\n",
        "\n",
        "# Training\n",
        "NUM_FRAMES  = 8      # frames per video  (reduce to 4 on T4)\n",
        "MAX_SEQ_LEN = 4096   # Aria supports up to 64K; 4K is safe for T4/A100\n",
        "EPOCHS      = 3\n",
        "BATCH_SIZE  = 1\n",
        "GRAD_ACCUM  = 8\n",
        "LORA_RANK   = 16\n",
        "EVAL_SPLIT  = 0.10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Paths\n",
        "OUTPUT_DIR          = \"./aria_finance\"\n",
        "ADAPTER_DRIVE       = \"/content/drive/MyDrive/IIT_Internship/aria_finance_adapter\"\n",
        "RESULTS_EXCEL_DRIVE = \"/content/drive/MyDrive/IIT_Internship/aria_finance_results.xlsx\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 4 â”€â”€ Imports\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "import os, re, json, cv2, shutil, torch, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rouge_score import rouge_scorer as rs\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"âœ… Imports OK\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 5 â”€â”€ Load Dataset\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    ds = load_dataset(HF_DATASET, split=\"train\")\n",
        "    df = ds.to_pandas()\n",
        "    print(f\"âœ… Loaded from HuggingFace: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  HF failed ({e}), loading from Excel...\")\n",
        "    df = pd.read_excel(EXCEL_FALLBACK)\n",
        "    print(f\"âœ… Loaded from Excel: {len(df)} rows\")\n",
        "\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "REQUIRED = [\"uni_id\",\"Category\",\"Aspect\",\"Intent\",\"Explanation\",\"Country\",\"Source\",\"Sector\"]\n",
        "missing  = [c for c in REQUIRED if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "df = df.dropna(subset=REQUIRED).reset_index(drop=True)\n",
        "print(f\"   Rows: {len(df)} | Categories: {df['Category'].unique().tolist()}\")\n",
        "\n",
        "ALL_ASPECTS = sorted(df[\"Aspect\"].unique().tolist())\n",
        "print(f\"   Aspects: {len(ALL_ASPECTS)} unique values\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 6 â”€â”€ Prompt\n",
        "#\n",
        "#  Aria message format (official):\n",
        "#    content is a LIST of dicts, each with \"type\" and either\n",
        "#    \"text\" (str) or image data.\n",
        "#    For images: {\"type\": \"image\"}  with None text field,\n",
        "#    then pass PIL image list to processor separately.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "def build_messages(n_frames: int, answer: str = None) -> list:\n",
        "    \"\"\"\n",
        "    Build Aria conversation messages.\n",
        "    Each frame gets its own {\"text\": None, \"type\": \"image\"} dict,\n",
        "    followed by the text prompt.\n",
        "    If answer is provided (training), includes assistant turn.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "\n",
        "    # One image entry per frame â€” Aria processes each frame independently\n",
        "    for i in range(n_frames):\n",
        "        content.append({\"text\": None, \"type\": \"image\"})\n",
        "\n",
        "    # Text instruction after all image tokens\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": f\"\"\"You are an expert video analyst specialising in financial harassment, fraud, abuse, and workplace misconduct.\n",
        "\n",
        "The {n_frames} images above are evenly-sampled frames from a video.\n",
        "\n",
        "Analyse the frames carefully and return a single valid JSON object with EXACTLY these fields:\n",
        "{{\n",
        "  \"category\"   : \"Harassment\" | \"Bullying\",\n",
        "  \"aspect\"     : one of the known aspect labels listed below,\n",
        "  \"intent\"     : 1 (intentional) | 0 (unintentional),\n",
        "  \"explanation\": \"1-2 sentence description of what is happening and why it is problematic\",\n",
        "  \"country\"    : \"country name or Unknown\",\n",
        "  \"source\"     : \"YouTube | News | Training | Documentary | Other\",\n",
        "  \"sector\"     : \"Finance | Healthcare | Education | Retail | Tech | Government | General\"\n",
        "}}\n",
        "\n",
        "Known aspect labels (pick the closest match):\n",
        "{json.dumps(ALL_ASPECTS, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the JSON. No preamble, no markdown fences, no extra text.\n",
        "- Use double quotes for all strings.\n",
        "- intent must be an integer (1 or 0), not a string.\n",
        "\n",
        "Observe carefully:\n",
        "â€¢ Who is involved and what is happening\n",
        "â€¢ The type and form of misconduct\n",
        "â€¢ Whether the act appears deliberate\n",
        "â€¢ Country and industry context clues\"\"\"\n",
        "    })\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "    if answer is not None:\n",
        "        messages.append({\n",
        "            \"role\"   : \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": answer}]\n",
        "        })\n",
        "\n",
        "    return messages\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 â”€â”€ Helpers\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "def find_video(uni_id: str) -> str:\n",
        "    filename = f\"{uni_id}{VIDEO_EXT}\"\n",
        "    for folder in VIDEO_DIRS:\n",
        "        path = os.path.join(folder, filename)\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "        if os.path.isdir(folder):\n",
        "            for f in os.listdir(folder):\n",
        "                if f.lower() == filename.lower():\n",
        "                    return os.path.join(folder, f)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_frames(video_path: str, n: int = NUM_FRAMES) -> list:\n",
        "    if not video_path or not os.path.exists(video_path):\n",
        "        return []\n",
        "    cap   = cv2.VideoCapture(video_path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    frames = []\n",
        "    for idx in np.linspace(0, total - 1, n, dtype=int):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, frame = cap.read()\n",
        "        if ok:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def row_to_json(row) -> str:\n",
        "    return json.dumps({\n",
        "        \"category\"   : str(row[\"Category\"]).strip(),\n",
        "        \"aspect\"     : str(row[\"Aspect\"]).strip(),\n",
        "        \"intent\"     : int(row[\"Intent\"]),\n",
        "        \"explanation\": str(row[\"Explanation\"]).strip(),\n",
        "        \"country\"    : str(row[\"Country\"]).strip(),\n",
        "        \"source\"     : str(row[\"Source\"]).strip(),\n",
        "        \"sector\"     : str(row[\"Sector\"]).strip(),\n",
        "    }, indent=2)\n",
        "\n",
        "\n",
        "def parse_json_response(text: str) -> dict:\n",
        "    try:\n",
        "        m = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if m:\n",
        "            return json.loads(m.group())\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 8 â”€â”€ Load Model + Processor + LoRA\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(f\"\\nLoading {MODEL_PATH} [{QUANTISATION}]...\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    trust_remote_code = True,\n",
        ")\n",
        "\n",
        "# Build quantisation config\n",
        "if QUANTISATION == \"4bit\":\n",
        "    quant_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit              = True,\n",
        "        bnb_4bit_quant_type       = \"nf4\",\n",
        "        bnb_4bit_use_double_quant = True,\n",
        "        bnb_4bit_compute_dtype    = torch.bfloat16,\n",
        "    )\n",
        "elif QUANTISATION == \"8bit\":\n",
        "    quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
        "else:\n",
        "    quant_cfg = None\n",
        "\n",
        "model_kwargs = dict(\n",
        "    trust_remote_code = True,\n",
        "    low_cpu_mem_usage = True,\n",
        "    device_map        = \"auto\",\n",
        ")\n",
        "if quant_cfg:\n",
        "    model_kwargs[\"quantization_config\"] = quant_cfg\n",
        "else:\n",
        "    model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, **model_kwargs)\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded\")\n",
        "\n",
        "# Apply LoRA to the MoE language backbone\n",
        "# Aria uses Mixtral-style MoE â€” target both attention and MoE expert projections\n",
        "lora_config = LoraConfig(\n",
        "    task_type      = TaskType.CAUSAL_LM,\n",
        "    r              = LORA_RANK,\n",
        "    lora_alpha     = LORA_RANK * 2,\n",
        "    lora_dropout   = 0.05,\n",
        "    # Target attention + MoE gate/expert projections\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention\n",
        "        \"w1\", \"w2\", \"w3\",                           # MoE expert FFN (Aria uses w1/w2/w3)\n",
        "    ],\n",
        "    bias = \"none\",\n",
        ")\n",
        "\n",
        "# Aria's language model is accessed via .language_model attribute\n",
        "model.language_model = get_peft_model(model.language_model, lora_config)\n",
        "model.language_model.print_trainable_parameters()\n",
        "print(\"âœ… LoRA applied to language_model (MoE backbone)\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 9 â”€â”€ Build Samples\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "train_df, eval_df = train_test_split(df, test_size=EVAL_SPLIT, random_state=RANDOM_SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "eval_df  = eval_df.reset_index(drop=True)\n",
        "print(f\"\\nSplit â†’ Train: {len(train_df)}  |  Eval: {len(eval_df)}\")\n",
        "\n",
        "\n",
        "def build_samples(dataframe, split_name=\"train\"):\n",
        "    samples, skipped = [], 0\n",
        "    for i, row in dataframe.iterrows():\n",
        "        uni_id     = str(row[\"uni_id\"]).strip()\n",
        "        video_path = find_video(uni_id)\n",
        "        frames     = extract_frames(video_path)\n",
        "\n",
        "        if not frames:\n",
        "            print(f\"  âš ï¸  [{split_name}] Skipping {uni_id} â€” video not found\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        target = row_to_json(row)\n",
        "        samples.append({\n",
        "            \"pil_images\" : frames,\n",
        "            \"messages\"   : build_messages(len(frames), answer=target),\n",
        "            \"target_json\": target,\n",
        "            \"uni_id\"     : uni_id,\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 30 == 0:\n",
        "            print(f\"  [{split_name}] {i+1}/{len(dataframe)}  skipped={skipped}\")\n",
        "\n",
        "    print(f\"  âœ… [{split_name}] ready={len(samples)}  skipped={skipped}\")\n",
        "    return samples\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“¦ Building training samples...\")\n",
        "train_samples = build_samples(train_df, \"train\")\n",
        "\n",
        "print(\"\\nğŸ“¦ Building eval samples...\")\n",
        "eval_samples  = build_samples(eval_df,  \"eval\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 10 â”€â”€ PyTorch Dataset + Collator\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "class AriaDataset(TorchDataset):\n",
        "    \"\"\"\n",
        "    Tokenises each sample using Aria's processor:\n",
        "      processor(text=chat_text, images=pil_list, return_tensors=\"pt\")\n",
        "\n",
        "    pixel_values must be cast to model.dtype before forward pass.\n",
        "    Labels mask the user/system portion so only the answer is trained.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, processor, max_length=4096):\n",
        "        self.samples    = samples\n",
        "        self.processor  = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s          = self.samples[idx]\n",
        "        pil_images = s[\"pil_images\"]\n",
        "        messages   = s[\"messages\"]   # includes assistant answer\n",
        "\n",
        "        # apply_chat_template â†’ full text string with special tokens\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt = False,\n",
        "        )\n",
        "\n",
        "        # Tokenise text + images together\n",
        "        inputs = self.processor(\n",
        "            text         = text,\n",
        "            images       = pil_images,\n",
        "            return_tensors = \"pt\",\n",
        "            max_length   = self.max_length,\n",
        "            truncation   = True,\n",
        "            padding      = False,\n",
        "        )\n",
        "\n",
        "        input_ids   = inputs[\"input_ids\"].squeeze(0)\n",
        "        pixel_values = inputs[\"pixel_values\"]   # shape varies with Aria's dynamic tiling\n",
        "\n",
        "        # Build labels â€” mask everything before assistant answer\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Find the assistant header token sequence to mask up to answer\n",
        "        # Aria uses \"<|im_start|>assistant\" as the turn separator\n",
        "        assistant_tokens = self.processor.tokenizer.encode(\n",
        "            \"<|im_start|>assistant\\n\", add_special_tokens=False\n",
        "        )\n",
        "        a_len     = len(assistant_tokens)\n",
        "        mask_until = 0\n",
        "        for pos in range(len(input_ids) - a_len):\n",
        "            if input_ids[pos : pos + a_len].tolist() == assistant_tokens:\n",
        "                mask_until = pos + a_len\n",
        "\n",
        "        labels[:mask_until] = -100\n",
        "\n",
        "        # Mask pad tokens\n",
        "        pad_id = self.processor.tokenizer.pad_token_id or self.processor.tokenizer.eos_token_id\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\"     : input_ids,\n",
        "            \"attention_mask\": torch.ones_like(input_ids),\n",
        "            \"labels\"        : labels,\n",
        "            \"pixel_values\"  : pixel_values,   # kept as-is; collator handles stacking\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id\n",
        "\n",
        "    input_ids      = pad_sequence([b[\"input_ids\"]      for b in batch], batch_first=True, padding_value=pad_id)\n",
        "    attention_mask = pad_sequence([b[\"attention_mask\"] for b in batch], batch_first=True, padding_value=0)\n",
        "    labels         = pad_sequence([b[\"labels\"]         for b in batch], batch_first=True, padding_value=-100)\n",
        "\n",
        "    # pixel_values: Aria uses dynamic tiling so shapes can differ between samples.\n",
        "    # Concatenate along batch dim when possible; fallback to list for variable shapes.\n",
        "    try:\n",
        "        pixel_values = torch.cat([b[\"pixel_values\"] for b in batch], dim=0)\n",
        "    except Exception:\n",
        "        pixel_values = batch[0][\"pixel_values\"]   # single-sample fallback\n",
        "\n",
        "    return {\n",
        "        \"input_ids\"     : input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\"        : labels,\n",
        "        \"pixel_values\"  : pixel_values,\n",
        "    }\n",
        "\n",
        "\n",
        "train_torch_ds = AriaDataset(train_samples, processor, max_length=MAX_SEQ_LEN)\n",
        "print(f\"âœ… PyTorch dataset ready: {len(train_torch_ds)} training samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 11 â”€â”€ Train\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "model.train()\n",
        "model.language_model.enable_input_require_grads()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                    = OUTPUT_DIR,\n",
        "    per_device_train_batch_size   = BATCH_SIZE,\n",
        "    gradient_accumulation_steps   = GRAD_ACCUM,\n",
        "    num_train_epochs              = EPOCHS,\n",
        "    learning_rate                 = 2e-4,\n",
        "    bf16                          = True,\n",
        "    logging_steps                 = 5,\n",
        "    save_steps                    = 100,\n",
        "    save_total_limit              = 2,\n",
        "    warmup_ratio                  = 0.03,\n",
        "    lr_scheduler_type             = \"cosine\",\n",
        "    optim                         = \"paged_adamw_8bit\",\n",
        "    seed                          = RANDOM_SEED,\n",
        "    remove_unused_columns         = False,\n",
        "    dataloader_pin_memory         = False,\n",
        "    gradient_checkpointing        = True,\n",
        "    gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
        "    report_to                     = \"none\",\n",
        ")\n",
        "\n",
        "# Custom Trainer to cast pixel_values to model dtype before forward pass\n",
        "class AriaTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Aria requires pixel_values in the same dtype as model weights\n",
        "        if \"pixel_values\" in inputs and inputs[\"pixel_values\"] is not None:\n",
        "            inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(\n",
        "                dtype = torch.bfloat16,\n",
        "                device = model.device,\n",
        "            )\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
        "\n",
        "\n",
        "trainer = AriaTrainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_torch_ds,\n",
        "    data_collator = collate_fn,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ Training started...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.language_model.save_pretrained(\"aria_lora_adapter\")\n",
        "processor.save_pretrained(\"aria_lora_adapter\")\n",
        "os.makedirs(ADAPTER_DRIVE, exist_ok=True)\n",
        "shutil.copytree(\"aria_lora_adapter\", ADAPTER_DRIVE, dirs_exist_ok=True)\n",
        "print(f\"âœ… LoRA adapter saved â†’ {ADAPTER_DRIVE}\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 12 â”€â”€ Inference Function\n",
        "#\n",
        "#  Official Aria inference pattern:\n",
        "#    1. apply_chat_template(messages) â†’ text string\n",
        "#    2. processor(text, images)       â†’ input tensors\n",
        "#    3. pixel_values.to(model.dtype)  â†’ dtype match (critical!)\n",
        "#    4. model.generate(**inputs, stop_strings=[\"<|im_end|>\"])\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "model.eval()\n",
        "\n",
        "def predict(pil_images: list) -> dict:\n",
        "    \"\"\"Run Aria inference on a list of PIL frames.\"\"\"\n",
        "    messages = build_messages(len(pil_images), answer=None)   # no answer for inference\n",
        "\n",
        "    text = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt = True,   # True for inference only\n",
        "    )\n",
        "\n",
        "    inputs = processor(\n",
        "        text   = text,\n",
        "        images = pil_images,\n",
        "        return_tensors = \"pt\",\n",
        "    )\n",
        "\n",
        "    # Cast pixel_values to model dtype â€” this is critical for Aria\n",
        "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(\n",
        "        dtype  = torch.bfloat16,\n",
        "        device = model.device,\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) if hasattr(v, \"to\") else v\n",
        "              for k, v in inputs.items()}\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens = 512,\n",
        "            do_sample      = False,\n",
        "            temperature    = 0.1,\n",
        "            stop_strings   = [\"<|im_end|>\"],          # Aria's stop token\n",
        "            tokenizer      = processor.tokenizer,     # required for stop_strings\n",
        "        )\n",
        "\n",
        "    # Decode only the newly generated tokens (exclude prompt)\n",
        "    new_tokens = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    response   = processor.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return parse_json_response(response)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 13 â”€â”€ Run Inference on ALL samples (train + eval)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"  ğŸ” Running inference on ALL samples (train + eval)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "all_samples = train_samples + eval_samples\n",
        "all_results = []\n",
        "\n",
        "for i, s in enumerate(all_samples):\n",
        "    pred = predict(s[\"pil_images\"])\n",
        "    tgt  = json.loads(s[\"target_json\"])\n",
        "\n",
        "    all_results.append({\n",
        "        \"uni_id\"          : s[\"uni_id\"],\n",
        "        \"split\"           : \"train\" if i < len(train_samples) else \"eval\",\n",
        "        \"true_category\"   : tgt.get(\"category\",    \"\"),\n",
        "        \"true_aspect\"     : tgt.get(\"aspect\",       \"\"),\n",
        "        \"true_intent\"     : str(tgt.get(\"intent\",   \"\")),\n",
        "        \"true_explanation\": tgt.get(\"explanation\",  \"\"),\n",
        "        \"true_country\"    : tgt.get(\"country\",      \"\"),\n",
        "        \"true_source\"     : tgt.get(\"source\",       \"\"),\n",
        "        \"true_sector\"     : tgt.get(\"sector\",       \"\"),\n",
        "        \"pred_category\"   : str(pred.get(\"category\",    \"\")).strip(),\n",
        "        \"pred_aspect\"     : str(pred.get(\"aspect\",       \"\")).strip(),\n",
        "        \"pred_intent\"     : str(pred.get(\"intent\",       \"\")).strip(),\n",
        "        \"pred_explanation\": str(pred.get(\"explanation\",  \"\")).strip(),\n",
        "        \"pred_country\"    : str(pred.get(\"country\",      \"\")).strip(),\n",
        "        \"pred_source\"     : str(pred.get(\"source\",       \"\")).strip(),\n",
        "        \"pred_sector\"     : str(pred.get(\"sector\",       \"\")).strip(),\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 20 == 0:\n",
        "        print(f\"  Inferred {i+1}/{len(all_samples)}\")\n",
        "\n",
        "print(f\"âœ… Inference done for {len(all_results)} samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 14 â”€â”€ Accuracy Scoring\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "scorer = rs.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def score_split(rows, label=\"ALL\"):\n",
        "    if not rows:\n",
        "        return {}\n",
        "\n",
        "    def acc(t, p):\n",
        "        return accuracy_score(\n",
        "            [x.lower().strip() for x in t],\n",
        "            [x.lower().strip() for x in p]\n",
        "        )\n",
        "\n",
        "    rouge_scores = [\n",
        "        scorer.score(r[\"true_explanation\"], r[\"pred_explanation\"])[\"rougeL\"].fmeasure\n",
        "        for r in rows\n",
        "    ]\n",
        "    avg_rouge = float(np.mean(rouge_scores))\n",
        "\n",
        "    cat_acc = acc([r[\"true_category\"] for r in rows], [r[\"pred_category\"] for r in rows])\n",
        "    asp_acc = acc([r[\"true_aspect\"]   for r in rows], [r[\"pred_aspect\"]   for r in rows])\n",
        "    int_acc = acc([r[\"true_intent\"]   for r in rows], [r[\"pred_intent\"]   for r in rows])\n",
        "    cty_acc = acc([r[\"true_country\"]  for r in rows], [r[\"pred_country\"]  for r in rows])\n",
        "    sec_acc = acc([r[\"true_sector\"]   for r in rows], [r[\"pred_sector\"]   for r in rows])\n",
        "    src_acc = acc([r[\"true_source\"]   for r in rows], [r[\"pred_source\"]   for r in rows])\n",
        "\n",
        "    overall = (cat_acc*0.25 + asp_acc*0.25 + int_acc*0.15 +\n",
        "               avg_rouge*0.20 + sec_acc*0.10 + cty_acc*0.03 + src_acc*0.02)\n",
        "\n",
        "    return {\n",
        "        \"split\"             : label,\n",
        "        \"n_samples\"         : len(rows),\n",
        "        \"category_acc\"      : round(cat_acc   * 100, 2),\n",
        "        \"aspect_acc\"        : round(asp_acc   * 100, 2),\n",
        "        \"intent_acc\"        : round(int_acc   * 100, 2),\n",
        "        \"country_acc\"       : round(cty_acc   * 100, 2),\n",
        "        \"source_acc\"        : round(src_acc   * 100, 2),\n",
        "        \"sector_acc\"        : round(sec_acc   * 100, 2),\n",
        "        \"explanation_rougeL\": round(avg_rouge * 100, 2),\n",
        "        \"OVERALL_SCORE_%\"   : round(overall   * 100, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "train_rows   = [r for r in all_results if r[\"split\"] == \"train\"]\n",
        "eval_rows    = [r for r in all_results if r[\"split\"] == \"eval\"]\n",
        "train_scores = score_split(train_rows,  \"TRAIN\")\n",
        "eval_scores  = score_split(eval_rows,   \"EVAL\")\n",
        "all_scores   = score_split(all_results, \"ALL\")\n",
        "\n",
        "def print_scores(s: dict):\n",
        "    print(f\"\\n  â”€â”€ {s['split']} SET  ({s['n_samples']} samples) â”€â”€\")\n",
        "    print(f\"   Category    Accuracy  : {s['category_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Aspect      Accuracy  : {s['aspect_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Intent      Accuracy  : {s['intent_acc']:>7.2f} %   (weight 15%)\")\n",
        "    print(f\"   Explanation ROUGE-L   : {s['explanation_rougeL']:>7.2f} %   (weight 20%)\")\n",
        "    print(f\"   Sector      Accuracy  : {s['sector_acc']:>7.2f} %   (weight 10%)\")\n",
        "    print(f\"   Country     Accuracy  : {s['country_acc']:>7.2f} %   (weight  3%)\")\n",
        "    print(f\"   Source      Accuracy  : {s['source_acc']:>7.2f} %   (weight  2%)\")\n",
        "    print(f\"   {'â”€'*42}\")\n",
        "    print(f\"   â­ OVERALL SCORE      : {s['OVERALL_SCORE_%']:>7.2f} %\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"              ğŸ“Š ACCURACY REPORT\")\n",
        "print(\"=\"*65)\n",
        "print_scores(train_scores)\n",
        "print_scores(eval_scores)\n",
        "print_scores(all_scores)\n",
        "print(\"=\"*65)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 15 â”€â”€ Save Results to Excel (7 sheets)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "results_df = pd.DataFrame(all_results)\n",
        "summary_df = pd.DataFrame([train_scores, eval_scores, all_scores])\n",
        "\n",
        "os.makedirs(os.path.dirname(RESULTS_EXCEL_DRIVE), exist_ok=True)\n",
        "\n",
        "with pd.ExcelWriter(RESULTS_EXCEL_DRIVE, engine=\"openpyxl\") as writer:\n",
        "    summary_df.to_excel(writer, sheet_name=\"Accuracy Summary\", index=False)\n",
        "    results_df.to_excel(writer, sheet_name=\"Per Sample\",       index=False)\n",
        "\n",
        "    for field in [\"category\", \"aspect\", \"intent\", \"sector\", \"country\", \"source\"]:\n",
        "        breakdown = results_df[[\n",
        "            \"uni_id\", \"split\",\n",
        "            f\"true_{field}\", f\"pred_{field}\"\n",
        "        ]].copy()\n",
        "        breakdown[\"correct\"] = (\n",
        "            breakdown[f\"true_{field}\"].str.lower().str.strip() ==\n",
        "            breakdown[f\"pred_{field}\"].str.lower().str.strip()\n",
        "        )\n",
        "        breakdown.to_excel(writer, sheet_name=f\"{field.capitalize()} Detail\", index=False)\n",
        "\n",
        "print(f\"\\nâœ… Results saved â†’ {RESULTS_EXCEL_DRIVE}\")\n",
        "print(\"\\nğŸ‰ ALL DONE!\")"
      ]
    }
  ]
}