{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPC6NW6VlLHBQYG0gSXVIWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekhyaGangopadhyay/IITPatna_Internship_LLMs/blob/main/DeepSeek_VL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIOYWp5p1WW4"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  DeepSeek-VL2  â–¸  Finance Harassment/Abuse Video Dataset\n",
        "#  Full Pipeline : Train â†’ Infer â†’ Accuracy Score\n",
        "#  Dataset       : iamalekhya/Finance_set  (318 videos, 9 cols)\n",
        "#  Run on        : Google Colab â†’ Runtime â–¸ T4 GPU\n",
        "#\n",
        "#  Model variants & VRAM needed:\n",
        "#    deepseek-vl2-tiny  â†’  ~14 GB  âœ… T4 (16 GB)   â† USE THIS\n",
        "#    deepseek-vl2-small â†’  ~40 GB  âš ï¸  A100 only\n",
        "#    deepseek-vl2       â†’  ~80 GB  âŒ  multi-GPU\n",
        "# ============================================================\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â”€â”€ Install  (run once â†’ Runtime â–¸ Restart session)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# DeepSeek-VL2 requires its own package + pinned transformers\n",
        "!pip install git+https://github.com/deepseek-ai/DeepSeek-VL2.git --no-deps\n",
        "!pip install attrdict timm \"transformers<4.48.0\"\n",
        "!pip install peft accelerate bitsandbytes rouge-score openpyxl opencv-python scikit-learn\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 2 â”€â”€ Mount Google Drive\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 3 â”€â”€ âš™ï¸  CONFIG  â† only edit this section\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "# Model â€” use tiny for T4, small for A100\n",
        "MODEL_PATH  = \"deepseek-ai/deepseek-vl2-tiny\"\n",
        "\n",
        "# Dataset\n",
        "HF_DATASET     = \"iamalekhya/Finance_set\"\n",
        "EXCEL_FALLBACK = \"/content/drive/MyDrive/IITP_Internship/Finance_set.xlsx\"\n",
        "\n",
        "# Videos in Google Drive (searches all folders)\n",
        "VIDEO_DIRS = [\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/AG\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/ag\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Alekhya\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Pritam\",\n",
        "    \"/content/drive/Shared drives/Finance_video_Anamoly/Videos\",\n",
        "]\n",
        "VIDEO_EXT = \".mp4\"\n",
        "\n",
        "# Training\n",
        "NUM_FRAMES  = 8       # frames extracted per video (keep â‰¤8 for T4)\n",
        "MAX_SEQ_LEN = 1024    # lower = less VRAM\n",
        "EPOCHS      = 3\n",
        "BATCH_SIZE  = 1\n",
        "GRAD_ACCUM  = 8\n",
        "LORA_RANK   = 8       # lower than Qwen since MoE already efficient\n",
        "EVAL_SPLIT  = 0.10\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_DIR          = \"./deepseek_vl2_finance\"\n",
        "ADAPTER_DRIVE       = \"/content/drive/MyDrive/IIT_Internship/deepseek_vl2_adapter\"\n",
        "RESULTS_EXCEL_DRIVE = \"/content/drive/MyDrive/IIT_Internship/deepseek_finance_results.xlsx\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 4 â”€â”€ Imports\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "import os, re, json, cv2, shutil, torch, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM\n",
        "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from rouge_score import rouge_scorer as rs\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"âœ… Imports OK\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 5 â”€â”€ Load Dataset\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    ds = load_dataset(HF_DATASET, split=\"train\")\n",
        "    df = ds.to_pandas()\n",
        "    print(f\"âœ… Loaded from HuggingFace: {len(df)} rows\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  HF failed ({e}), loading from Excel...\")\n",
        "    df = pd.read_excel(EXCEL_FALLBACK)\n",
        "    print(f\"âœ… Loaded from Excel: {len(df)} rows\")\n",
        "\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "REQUIRED = [\"uni_id\",\"Category\",\"Aspect\",\"Intent\",\"Explanation\",\"Country\",\"Source\",\"Sector\"]\n",
        "missing  = [c for c in REQUIRED if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "df = df.dropna(subset=REQUIRED).reset_index(drop=True)\n",
        "print(f\"   Rows: {len(df)} | Categories: {df['Category'].unique().tolist()}\")\n",
        "\n",
        "ALL_ASPECTS = sorted(df[\"Aspect\"].unique().tolist())\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 6 â”€â”€ Prompts  (DeepSeek-VL2 style)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# DeepSeek-VL2 uses <image> token per image in content string\n",
        "# and roles must be exactly \"<|User|>\" and \"<|Assistant|>\"\n",
        "\n",
        "def build_user_content(num_frames: int) -> str:\n",
        "    \"\"\"Build the user message string with <image> tokens for each frame.\"\"\"\n",
        "    image_tags = \"\\n\".join([f\"Frame {i+1}: <image>\" for i in range(num_frames)])\n",
        "    return f\"\"\"{image_tags}\n",
        "\n",
        "You are an expert video analyst specialising in financial harassment, fraud, abuse, and workplace misconduct.\n",
        "\n",
        "Analyse the video frames above and return a single valid JSON object with EXACTLY these fields:\n",
        "{{\n",
        "  \"category\"   : \"Harassment\" | \"Bullying\",\n",
        "  \"aspect\"     : one of the known aspect labels listed below,\n",
        "  \"intent\"     : 1 (intentional) | 0 (unintentional),\n",
        "  \"explanation\": \"1-2 sentence description of what is happening and why it is problematic\",\n",
        "  \"country\"    : \"country name or Unknown\",\n",
        "  \"source\"     : \"YouTube | News | Training | Documentary | Other\",\n",
        "  \"sector\"     : \"Finance | Healthcare | Education | Retail | Tech | Government | General\"\n",
        "}}\n",
        "\n",
        "Known aspect labels (pick the closest match):\n",
        "{json.dumps(ALL_ASPECTS, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the JSON. No preamble, no markdown fences, no extra text.\n",
        "- Use double quotes for all strings.\n",
        "- intent must be an integer (1 or 0), not a string.\n",
        "\n",
        "Carefully observe:\n",
        "â€¢ Who is involved and what is happening\n",
        "â€¢ The type and form of misconduct\n",
        "â€¢ Whether the act is intentional\n",
        "â€¢ The country/industry context\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = \"You are an expert video analyst specialising in financial harassment, fraud, abuse, and workplace misconduct. Always respond with valid JSON only.\"\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 â”€â”€ Helpers\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "def find_video(uni_id: str) -> str:\n",
        "    filename = f\"{uni_id}{VIDEO_EXT}\"\n",
        "    for folder in VIDEO_DIRS:\n",
        "        path = os.path.join(folder, filename)\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "        if os.path.isdir(folder):\n",
        "            for f in os.listdir(folder):\n",
        "                if f.lower() == filename.lower():\n",
        "                    return os.path.join(folder, f)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_frames(video_path: str, n: int = NUM_FRAMES) -> list:\n",
        "    if not video_path or not os.path.exists(video_path):\n",
        "        return []\n",
        "    cap   = cv2.VideoCapture(video_path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total == 0:\n",
        "        cap.release()\n",
        "        return []\n",
        "    frames = []\n",
        "    for idx in np.linspace(0, total - 1, n, dtype=int):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ok, frame = cap.read()\n",
        "        if ok:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def row_to_json(row) -> str:\n",
        "    return json.dumps({\n",
        "        \"category\"   : str(row[\"Category\"]).strip(),\n",
        "        \"aspect\"     : str(row[\"Aspect\"]).strip(),\n",
        "        \"intent\"     : int(row[\"Intent\"]),\n",
        "        \"explanation\": str(row[\"Explanation\"]).strip(),\n",
        "        \"country\"    : str(row[\"Country\"]).strip(),\n",
        "        \"source\"     : str(row[\"Source\"]).strip(),\n",
        "        \"sector\"     : str(row[\"Sector\"]).strip(),\n",
        "    }, indent=2)\n",
        "\n",
        "\n",
        "def parse_json_response(text: str) -> dict:\n",
        "    try:\n",
        "        m = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if m:\n",
        "            return json.loads(m.group())\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 8 â”€â”€ Load Model + Processor + LoRA\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(f\"\\nLoading {MODEL_PATH}...\")\n",
        "\n",
        "# DeepSeek-VL2 uses its own processor (not HuggingFace AutoProcessor)\n",
        "vl_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(\n",
        "    MODEL_PATH, trust_remote_code=True\n",
        ")\n",
        "tokenizer = vl_processor.tokenizer\n",
        "\n",
        "# Load model in bfloat16 (4-bit not officially supported for DeepSeek-VL2)\n",
        "vl_model: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    trust_remote_code = True,\n",
        "    torch_dtype       = torch.bfloat16,\n",
        "    device_map        = \"auto\",        # auto splits across available GPU(s)\n",
        ")\n",
        "vl_model.eval()\n",
        "\n",
        "# Apply LoRA to language backbone only\n",
        "# DeepSeek-VL2 language model is accessed via .language attribute\n",
        "lora_config = LoraConfig(\n",
        "    task_type        = TaskType.CAUSAL_LM,\n",
        "    r                = LORA_RANK,\n",
        "    lora_alpha       = LORA_RANK * 2,\n",
        "    lora_dropout     = 0.05,\n",
        "    target_modules   = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias             = \"none\",\n",
        ")\n",
        "vl_model.language = get_peft_model(vl_model.language, lora_config)\n",
        "vl_model.language.print_trainable_parameters()\n",
        "print(\"âœ… Model + LoRA ready\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 9 â”€â”€ Build Samples\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "train_df, eval_df = train_test_split(df, test_size=EVAL_SPLIT, random_state=RANDOM_SEED)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "eval_df  = eval_df.reset_index(drop=True)\n",
        "print(f\"\\nSplit â†’ Train: {len(train_df)}  |  Eval: {len(eval_df)}\")\n",
        "\n",
        "\n",
        "def build_samples(dataframe, split_name=\"train\"):\n",
        "    \"\"\"\n",
        "    Each sample stores:\n",
        "      - pil_images : list of PIL frames\n",
        "      - conversation: DeepSeek-VL2 conversation dict format\n",
        "      - target_json : ground truth answer string\n",
        "      - uni_id      : video id\n",
        "    \"\"\"\n",
        "    samples, skipped = [], 0\n",
        "\n",
        "    for i, row in dataframe.iterrows():\n",
        "        uni_id     = str(row[\"uni_id\"]).strip()\n",
        "        video_path = find_video(uni_id)\n",
        "        frames     = extract_frames(video_path)\n",
        "\n",
        "        if not frames:\n",
        "            print(f\"  âš ï¸  [{split_name}] Skipping {uni_id} â€” not found\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        n_frames    = len(frames)\n",
        "        target      = row_to_json(row)\n",
        "        user_text   = build_user_content(n_frames)\n",
        "\n",
        "        # DeepSeek-VL2 conversation format\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\"   : \"<|User|>\",\n",
        "                \"content\": user_text,\n",
        "                \"images\" : frames,      # list of PIL Images directly\n",
        "            },\n",
        "            {\n",
        "                \"role\"   : \"<|Assistant|>\",\n",
        "                \"content\": target,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        samples.append({\n",
        "            \"conversation\": conversation,\n",
        "            \"pil_images\"  : frames,\n",
        "            \"target_json\" : target,\n",
        "            \"uni_id\"      : uni_id,\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 30 == 0:\n",
        "            print(f\"  [{split_name}] {i+1}/{len(dataframe)}  skipped={skipped}\")\n",
        "\n",
        "    print(f\"  âœ… [{split_name}] ready={len(samples)}  skipped={skipped}\")\n",
        "    return samples\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“¦ Building training samples...\")\n",
        "train_samples = build_samples(train_df, \"train\")\n",
        "\n",
        "print(\"\\nğŸ“¦ Building eval samples...\")\n",
        "eval_samples  = build_samples(eval_df,  \"eval\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 10 â”€â”€ PyTorch Dataset + Collator\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "class DeepSeekVL2Dataset(TorchDataset):\n",
        "    \"\"\"\n",
        "    Tokenises each sample using DeepseekVLV2Processor on-the-fly.\n",
        "    The processor handles <image> token expansion and vision encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, processor, max_length=1024):\n",
        "        self.samples    = samples\n",
        "        self.processor  = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s            = self.samples[idx]\n",
        "        conversation = s[\"conversation\"]\n",
        "        pil_images   = s[\"pil_images\"]\n",
        "\n",
        "        # vl_processor prepares input_ids, attention_mask, pixel_values etc.\n",
        "        inputs = self.processor(\n",
        "            conversations  = conversation,\n",
        "            images         = pil_images,\n",
        "            force_batchify = True,\n",
        "            system_prompt  = SYSTEM_PROMPT,\n",
        "        )\n",
        "\n",
        "        input_ids = inputs.input_ids.squeeze(0)\n",
        "\n",
        "        # Truncate if too long\n",
        "        if input_ids.shape[0] > self.max_length:\n",
        "            input_ids = input_ids[:self.max_length]\n",
        "\n",
        "        # Labels: mask everything except the assistant response with -100\n",
        "        # Find assistant response start by locating eos of user turn\n",
        "        labels = input_ids.clone()\n",
        "        # Find the separator between user and assistant in token ids\n",
        "        # DeepSeek uses <|Assistant|> token; mask user portion\n",
        "        assistant_token_id = self.processor.tokenizer.convert_tokens_to_ids(\"<|Assistant|>\")\n",
        "        assist_positions   = (input_ids == assistant_token_id).nonzero(as_tuple=True)[0]\n",
        "        if len(assist_positions) > 0:\n",
        "            first_assist = assist_positions[0].item()\n",
        "            labels[:first_assist + 1] = -100   # mask user + assistant header\n",
        "\n",
        "        # Mask pad tokens\n",
        "        pad_id = self.processor.tokenizer.pad_token_id or 0\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\"     : input_ids,\n",
        "            \"attention_mask\": (input_ids != pad_id).long(),\n",
        "            \"labels\"        : labels,\n",
        "        }\n",
        "\n",
        "        # Add pixel_values if present\n",
        "        if hasattr(inputs, \"pixel_values\") and inputs.pixel_values is not None:\n",
        "            item[\"pixel_values\"] = inputs.pixel_values.squeeze(0)\n",
        "\n",
        "        if hasattr(inputs, \"images_seq_mask\") and inputs.images_seq_mask is not None:\n",
        "            item[\"images_seq_mask\"] = inputs.images_seq_mask.squeeze(0)\n",
        "\n",
        "        if hasattr(inputs, \"images_emb_mask\") and inputs.images_emb_mask is not None:\n",
        "            item[\"images_emb_mask\"] = inputs.images_emb_mask.squeeze(0)\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = tokenizer.pad_token_id or 0\n",
        "\n",
        "    input_ids      = pad_sequence([b[\"input_ids\"]      for b in batch], batch_first=True, padding_value=pad_id)\n",
        "    attention_mask = pad_sequence([b[\"attention_mask\"] for b in batch], batch_first=True, padding_value=0)\n",
        "    labels         = pad_sequence([b[\"labels\"]         for b in batch], batch_first=True, padding_value=-100)\n",
        "\n",
        "    result = {\n",
        "        \"input_ids\"     : input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\"        : labels,\n",
        "    }\n",
        "\n",
        "    if \"pixel_values\" in batch[0]:\n",
        "        try:\n",
        "            result[\"pixel_values\"] = torch.stack([b[\"pixel_values\"] for b in batch])\n",
        "        except Exception:\n",
        "            pass  # skip if shapes differ (dynamic tiling)\n",
        "\n",
        "    if \"images_seq_mask\" in batch[0]:\n",
        "        result[\"images_seq_mask\"] = pad_sequence(\n",
        "            [b[\"images_seq_mask\"] for b in batch], batch_first=True, padding_value=False\n",
        "        )\n",
        "\n",
        "    if \"images_emb_mask\" in batch[0]:\n",
        "        result[\"images_emb_mask\"] = pad_sequence(\n",
        "            [b[\"images_emb_mask\"] for b in batch], batch_first=True, padding_value=False\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "train_torch_ds = DeepSeekVL2Dataset(train_samples, vl_processor, max_length=MAX_SEQ_LEN)\n",
        "print(f\"âœ… PyTorch train dataset: {len(train_torch_ds)} samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 11 â”€â”€ Train\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "vl_model.train()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                  = OUTPUT_DIR,\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    gradient_accumulation_steps = GRAD_ACCUM,\n",
        "    num_train_epochs            = EPOCHS,\n",
        "    learning_rate               = 2e-4,\n",
        "    bf16                        = True,\n",
        "    logging_steps               = 5,\n",
        "    save_steps                  = 100,\n",
        "    save_total_limit            = 2,\n",
        "    warmup_ratio                = 0.03,\n",
        "    lr_scheduler_type           = \"cosine\",\n",
        "    optim                       = \"adamw_torch\",\n",
        "    seed                        = RANDOM_SEED,\n",
        "    remove_unused_columns       = False,\n",
        "    dataloader_pin_memory       = False,\n",
        "    report_to                   = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model         = vl_model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_torch_ds,\n",
        "    data_collator = collate_fn,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸš€ Training started...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")\n",
        "\n",
        "# Save LoRA adapter\n",
        "vl_model.language.save_pretrained(\"deepseek_vl2_lora_adapter\")\n",
        "tokenizer.save_pretrained(\"deepseek_vl2_lora_adapter\")\n",
        "os.makedirs(ADAPTER_DRIVE, exist_ok=True)\n",
        "shutil.copytree(\"deepseek_vl2_lora_adapter\", ADAPTER_DRIVE, dirs_exist_ok=True)\n",
        "print(f\"âœ… LoRA adapter saved â†’ {ADAPTER_DRIVE}\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 12 â”€â”€ Inference Function\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "vl_model.eval()\n",
        "\n",
        "def predict(frames: list) -> dict:\n",
        "    \"\"\"\n",
        "    Run DeepSeek-VL2 inference on a list of PIL frames.\n",
        "    Returns parsed JSON dict with predicted fields.\n",
        "    \"\"\"\n",
        "    n_frames     = len(frames)\n",
        "    user_content = build_user_content(n_frames)\n",
        "\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\"   : \"<|User|>\",\n",
        "            \"content\": user_content,\n",
        "            \"images\" : frames,\n",
        "        },\n",
        "        {\n",
        "            \"role\"   : \"<|Assistant|>\",\n",
        "            \"content\": \"\",             # empty = model will fill this\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Prepare inputs using DeepSeek's processor\n",
        "    prepare_inputs = vl_processor(\n",
        "        conversations  = conversation,\n",
        "        images         = frames,\n",
        "        force_batchify = True,\n",
        "        system_prompt  = SYSTEM_PROMPT,\n",
        "    ).to(vl_model.device)\n",
        "\n",
        "    # Get image embeddings + run generation\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = vl_model.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "        outputs = vl_model.language.generate(\n",
        "            inputs_embeds  = inputs_embeds,\n",
        "            attention_mask = prepare_inputs.attention_mask,\n",
        "            pad_token_id   = tokenizer.eos_token_id,\n",
        "            bos_token_id   = tokenizer.bos_token_id,\n",
        "            eos_token_id   = tokenizer.eos_token_id,\n",
        "            max_new_tokens = 512,\n",
        "            temperature    = 0.1,\n",
        "            do_sample      = False,\n",
        "            use_cache      = True,\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "    return parse_json_response(answer)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 13 â”€â”€ Run Inference on ALL samples (train + eval)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"  ğŸ” Running inference on ALL samples (train + eval)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "all_samples = train_samples + eval_samples\n",
        "all_results = []\n",
        "\n",
        "for i, s in enumerate(all_samples):\n",
        "    pred = predict(s[\"pil_images\"])\n",
        "    tgt  = json.loads(s[\"target_json\"])\n",
        "\n",
        "    all_results.append({\n",
        "        \"uni_id\"          : s[\"uni_id\"],\n",
        "        \"split\"           : \"train\" if i < len(train_samples) else \"eval\",\n",
        "        \"true_category\"   : tgt.get(\"category\",    \"\"),\n",
        "        \"true_aspect\"     : tgt.get(\"aspect\",       \"\"),\n",
        "        \"true_intent\"     : str(tgt.get(\"intent\",   \"\")),\n",
        "        \"true_explanation\": tgt.get(\"explanation\",  \"\"),\n",
        "        \"true_country\"    : tgt.get(\"country\",      \"\"),\n",
        "        \"true_source\"     : tgt.get(\"source\",       \"\"),\n",
        "        \"true_sector\"     : tgt.get(\"sector\",       \"\"),\n",
        "        \"pred_category\"   : str(pred.get(\"category\",    \"\")).strip(),\n",
        "        \"pred_aspect\"     : str(pred.get(\"aspect\",       \"\")).strip(),\n",
        "        \"pred_intent\"     : str(pred.get(\"intent\",       \"\")).strip(),\n",
        "        \"pred_explanation\": str(pred.get(\"explanation\",  \"\")).strip(),\n",
        "        \"pred_country\"    : str(pred.get(\"country\",      \"\")).strip(),\n",
        "        \"pred_source\"     : str(pred.get(\"source\",       \"\")).strip(),\n",
        "        \"pred_sector\"     : str(pred.get(\"sector\",       \"\")).strip(),\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 20 == 0:\n",
        "        print(f\"  Inferred {i+1}/{len(all_samples)}\")\n",
        "\n",
        "print(f\"âœ… Inference done for {len(all_results)} samples\")\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 14 â”€â”€ Accuracy Scoring\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "scorer = rs.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def score_split(rows, label=\"ALL\"):\n",
        "    if not rows:\n",
        "        return {}\n",
        "\n",
        "    def acc(t, p):\n",
        "        return accuracy_score(\n",
        "            [x.lower().strip() for x in t],\n",
        "            [x.lower().strip() for x in p]\n",
        "        )\n",
        "\n",
        "    rouge_scores = [\n",
        "        scorer.score(r[\"true_explanation\"], r[\"pred_explanation\"])[\"rougeL\"].fmeasure\n",
        "        for r in rows\n",
        "    ]\n",
        "    avg_rouge = float(np.mean(rouge_scores))\n",
        "\n",
        "    cat_acc = acc([r[\"true_category\"] for r in rows], [r[\"pred_category\"] for r in rows])\n",
        "    asp_acc = acc([r[\"true_aspect\"]   for r in rows], [r[\"pred_aspect\"]   for r in rows])\n",
        "    int_acc = acc([r[\"true_intent\"]   for r in rows], [r[\"pred_intent\"]   for r in rows])\n",
        "    cty_acc = acc([r[\"true_country\"]  for r in rows], [r[\"pred_country\"]  for r in rows])\n",
        "    sec_acc = acc([r[\"true_sector\"]   for r in rows], [r[\"pred_sector\"]   for r in rows])\n",
        "    src_acc = acc([r[\"true_source\"]   for r in rows], [r[\"pred_source\"]   for r in rows])\n",
        "\n",
        "    overall = (cat_acc*0.25 + asp_acc*0.25 + int_acc*0.15 +\n",
        "               avg_rouge*0.20 + sec_acc*0.10 + cty_acc*0.03 + src_acc*0.02)\n",
        "\n",
        "    return {\n",
        "        \"split\"             : label,\n",
        "        \"n_samples\"         : len(rows),\n",
        "        \"category_acc\"      : round(cat_acc   * 100, 2),\n",
        "        \"aspect_acc\"        : round(asp_acc   * 100, 2),\n",
        "        \"intent_acc\"        : round(int_acc   * 100, 2),\n",
        "        \"country_acc\"       : round(cty_acc   * 100, 2),\n",
        "        \"source_acc\"        : round(src_acc   * 100, 2),\n",
        "        \"sector_acc\"        : round(sec_acc   * 100, 2),\n",
        "        \"explanation_rougeL\": round(avg_rouge * 100, 2),\n",
        "        \"OVERALL_SCORE_%\"   : round(overall   * 100, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "train_rows   = [r for r in all_results if r[\"split\"] == \"train\"]\n",
        "eval_rows    = [r for r in all_results if r[\"split\"] == \"eval\"]\n",
        "train_scores = score_split(train_rows,  \"TRAIN\")\n",
        "eval_scores  = score_split(eval_rows,   \"EVAL\")\n",
        "all_scores   = score_split(all_results, \"ALL\")\n",
        "\n",
        "def print_scores(s):\n",
        "    print(f\"\\n  â”€â”€ {s['split']} SET  ({s['n_samples']} samples) â”€â”€\")\n",
        "    print(f\"   Category    Accuracy  : {s['category_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Aspect      Accuracy  : {s['aspect_acc']:>7.2f} %   (weight 25%)\")\n",
        "    print(f\"   Intent      Accuracy  : {s['intent_acc']:>7.2f} %   (weight 15%)\")\n",
        "    print(f\"   Explanation ROUGE-L   : {s['explanation_rougeL']:>7.2f} %   (weight 20%)\")\n",
        "    print(f\"   Sector      Accuracy  : {s['sector_acc']:>7.2f} %   (weight 10%)\")\n",
        "    print(f\"   Country     Accuracy  : {s['country_acc']:>7.2f} %   (weight  3%)\")\n",
        "    print(f\"   Source      Accuracy  : {s['source_acc']:>7.2f} %   (weight  2%)\")\n",
        "    print(f\"   {'â”€'*42}\")\n",
        "    print(f\"   â­ OVERALL SCORE      : {s['OVERALL_SCORE_%']:>7.2f} %\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"              ğŸ“Š ACCURACY REPORT\")\n",
        "print(\"=\"*65)\n",
        "print_scores(train_scores)\n",
        "print_scores(eval_scores)\n",
        "print_scores(all_scores)\n",
        "print(\"=\"*65)\n",
        "\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 15 â”€â”€ Save Results to Excel\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "results_df = pd.DataFrame(all_results)\n",
        "summary_df = pd.DataFrame([train_scores, eval_scores, all_scores])\n",
        "\n",
        "os.makedirs(os.path.dirname(RESULTS_EXCEL_DRIVE), exist_ok=True)\n",
        "\n",
        "with pd.ExcelWriter(RESULTS_EXCEL_DRIVE, engine=\"openpyxl\") as writer:\n",
        "    summary_df.to_excel(writer, sheet_name=\"Accuracy Summary\", index=False)\n",
        "    results_df.to_excel(writer, sheet_name=\"Per Sample\",       index=False)\n",
        "\n",
        "    for field in [\"category\", \"aspect\", \"intent\", \"sector\", \"country\", \"source\"]:\n",
        "        breakdown = results_df[[\"uni_id\",\"split\",f\"true_{field}\",f\"pred_{field}\"]].copy()\n",
        "        breakdown[\"correct\"] = (\n",
        "            breakdown[f\"true_{field}\"].str.lower().str.strip() ==\n",
        "            breakdown[f\"pred_{field}\"].str.lower().str.strip()\n",
        "        )\n",
        "        breakdown.to_excel(writer, sheet_name=f\"{field.capitalize()} Detail\", index=False)\n",
        "\n",
        "print(f\"\\nâœ… Results saved â†’ {RESULTS_EXCEL_DRIVE}\")\n",
        "print(\"\\nğŸ‰ ALL DONE!\")"
      ]
    }
  ]
}